============================= test session starts ==============================
platform darwin -- Python 3.11.3, pytest-7.4.3, pluggy-1.6.0
Fugue tests will be initialized with options:
rootdir: /Users/jadenfix/eth
configfile: pyproject.toml
plugins: asyncio-0.21.1, respx-0.22.0, cov-5.0.0, sugar-1.0.0, html-4.1.1, env-1.1.5, timeout-2.4.0, metadata-3.1.1, jaxtyping-0.3.2, clarity-1.0.1, fugue-0.9.1, mock-3.12.0, anyio-3.7.1, docker-2.0.1, typeguard-4.4.4, xdist-3.6.1
asyncio: mode=Mode.STRICT
collected 92 items

tests/e2e/test_comprehensive.py ...F...F....FF..FFF.FF.FFF
tests/e2e/test_neo4j_connection.py F
tests/e2e/test_pipeline.py ..........
tests/e2e/tier0/test_t0_a_basic_ingestion.py FFF
tests/e2e/tier0/test_t0_b_basic_queries.py FFF
tests/e2e/tier0/test_t0_c_graph_queries.py FFFF
tests/e2e/tier0/test_t0_d_ui_rendering.py FFFFF‚ö†Ô∏è  T0-D: WebSocket not available (optional for T0)
.‚úÖ T0-D: UI config next.config.js is valid
‚úÖ T0-D: UI config package.json is valid
‚úÖ T0-D: UI config layout.json is valid
‚úÖ T0-D: UI configuration validation complete
.
tests/e2e/tier0/test_t0_simple_infrastructure.py ‚úÖ Python environment validation passed
.‚úÖ Pytest markers validation passed
.‚úÖ Test fixtures validation passed
.‚úÖ Time operations validation passed (duration: 0.0000s)
.‚úÖ Data structures validation passed
.‚úÖ Error handling validation passed
.‚úÖ List operations validation passed
.‚úÖ Configuration loading validation passed
.‚úÖ Async compatibility validation passed
.
tests/e2e/tier1/test_t1_a_realtime_ingestion.py FFFFF
tests/e2e/tier1/test_t1_b_bidirectional_sync.py FFFFF
tests/e2e/tier2/test_gcp_permissions_check.py Testing BigQuery permissions for project: ethhackathon
‚úÖ Dataset access successful: onchain_data
‚úÖ Tables found: ['action_executions', 'curated_events', 'entities', 'raw_data', 'raw_events', 'signal_explanations', 'transactions']
‚úÖ Query permissions working: 2025-07-25 20:21:03.366350+00:00
‚úÖ Table creation successful: permission_test_1753474864
‚úÖ Data insertion successful
‚úÖ Data query after insert successful
‚úÖ Table cleanup successful
.Testing Vertex AI permissions for project: ethhackathon, region: us-central1
‚úÖ Vertex AI initialization successful
‚úÖ Authentication successful for project: ethhackathon
Testing configured endpoint: https://us-central1-aiplatform.googleapis.com/v1/projects/ethhackathon/locations/us-central1/publishers/google/models/gemini-1.5-flash-lite:predict
‚úÖ Vertex AI endpoint accessible: 404
‚úÖ Vertex AI endpoint permissions working
.
============================================================
COMPREHENSIVE GCP PERMISSIONS TEST
============================================================

1. Testing BigQuery Permissions...
Testing BigQuery permissions for project: ethhackathon
‚úÖ Dataset access successful: onchain_data
‚úÖ Tables found: ['action_executions', 'curated_events', 'entities', 'raw_data', 'raw_events', 'signal_explanations', 'transactions']
‚úÖ Query permissions working: 2025-07-25 20:21:07.801249+00:00
‚úÖ Table creation successful: permission_test_1753474868
‚úÖ Data insertion successful
‚úÖ Data query after insert successful
‚úÖ Table cleanup successful

2. Testing Vertex AI Permissions...
Testing Vertex AI permissions for project: ethhackathon, region: us-central1
‚úÖ Vertex AI initialization successful
‚úÖ Authentication successful for project: ethhackathon
Testing configured endpoint: https://us-central1-aiplatform.googleapis.com/v1/projects/ethhackathon/locations/us-central1/publishers/google/models/gemini-1.5-flash-lite:predict
‚úÖ Vertex AI endpoint accessible: 404
‚úÖ Vertex AI endpoint permissions working

============================================================
PERMISSION TEST SUMMARY
============================================================
‚úÖ BigQuery: FULL ACCESS (read/write/create/delete)
‚úÖ Vertex AI: ACCESSIBLE (model endpoints available)

üéâ ALL GCP PERMISSIONS WORKING!
Ready for full V3 system deployment
.
tests/e2e/tier2/test_t2_a_real_service_integration.py ‚úÖ BigQuery client initialized for project: ethhackathon
‚úÖ BigQuery dataset configuration validated: onchain_data
‚úÖ BigQuery tables configured: raw_events, curated_events
‚úÖ Pub/Sub client initialized - Topic: raw-chain-events
‚úÖ Pub/Sub topic configuration validated
.F‚úÖ Alchemy API test passed - Latest block: 0x15eede8
‚úÖ Infura API test passed - Gas price: 0x1100d6c4
‚ö†Ô∏è  TheGraph API test limited: assert 'data' in {'errors': [{'message': 'subgraph not found: ELUcwgpm14LKPLrBRuVvPvNKHQ9HvwmtKgKSH6123cr7'}]}
‚úÖ TheGraph API configuration validated
.‚úÖ ElevenLabs integration test passed
.‚úÖ Vertex AI configured endpoint accessibility test passed
.‚úÖ Slack integration test passed - Team: Jaden Fix
.‚úÖ Stripe integration test passed
.Fs‚úÖ All environment configurations are properly set
.
tests/e2e/tier2/test_t2_b_v3_patches_integration.py FF‚úÖ Patch 3: Gemini Explainer test passed
.FFF

=================================== FAILURES ===================================
_____________ TestLayer1Ingestion.test_ethereum_ingestion_pipeline _____________

self = <tests.e2e.test_comprehensive.TestLayer1Ingestion object at 0x139814610>
mock_blockchain_data = {'block_number': 18500000, 'timestamp': 1753474840, 'transactions': [{'from': '0xabc1230000000000000000000000000000000...'from': '0xmev_bot000000000000000000000000000000', 'gas': 500000, 'gasPrice': '200000000000', 'gasUsed': 450000, ...}]}

    @pytest.mark.asyncio
    async def test_ethereum_ingestion_pipeline(self, mock_blockchain_data):
        """Test complete Ethereum ingestion pipeline."""
        # Mock Web3 and Pub/Sub
        with patch('services.ethereum_ingester.ethereum_ingester.Web3') as mock_web3, \
             patch('google.cloud.pubsub_v1.PublisherClient') as mock_publisher:
    
            from services.ethereum_ingester.ethereum_ingester import EthereumIngester
    
            # Setup mocks
            mock_web3_instance = Mock()
            mock_web3_instance.eth.block_number = mock_blockchain_data['block_number']
            mock_web3_instance.eth.get_block.return_value = Mock(
                number=mock_blockchain_data['block_number'],
                timestamp=mock_blockchain_data['timestamp'],
                transactions=[Mock(**tx) for tx in mock_blockchain_data['transactions']]
            )
            mock_web3.return_value = mock_web3_instance
    
            mock_pub_client = Mock()
            mock_pub_client.publish.return_value = Mock()
            mock_publisher.return_value = mock_pub_client
    
            # Test ingester
            ingester = EthereumIngester()
            await ingester._process_block(mock_blockchain_data['block_number'])
    
            # Verify events were published
>           assert mock_pub_client.publish.called
E           AssertionError: assert False
E            +  where False = <Mock name='PublisherClient().publish' id='5312246224'>.called
E            +    where <Mock name='PublisherClient().publish' id='5312246224'> = <Mock name='PublisherClient()' id='5312245712'>.publish

tests/e2e/test_comprehensive.py:200: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:40.206973Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0x1234567890abcdef000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:40.207092Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:40.207168Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0xfedcba0987654321000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:40.207208Z"}
______________ TestLayer2SemanticFusion.test_ontology_graphql_api ______________

self = <tests.e2e.test_comprehensive.TestLayer2SemanticFusion object at 0x1398160d0>

    def test_ontology_graphql_api(self):
        """Test ontology GraphQL API."""
        from services.graph_api.graph_api import app
    
        client = TestClient(app)
    
        # Test basic schema query
        query = """
        query {
            __schema {
                types {
                    name
                }
            }
        }
        """
    
        response = client.post("/graphql", json={"query": query})
        assert response.status_code == 200
    
        data = response.json()
        type_names = [t['name'] for t in data['data']['__schema']['types']]
    
        # Verify core types exist
        assert 'Entity' in type_names
>       assert 'Address' in type_names
E       AssertionError: assert 'Address' in ['Query', 'String', 'Int', 'Mutation', 'Entity', 'Float', ...]

tests/e2e/test_comprehensive.py:302: AssertionError
_________ TestLayer3IntelligenceAgentMesh.test_vertex_ai_pipeline_mock _________

self = <tests.e2e.test_comprehensive.TestLayer3IntelligenceAgentMesh object at 0x139824350>

    @pytest.mark.asyncio
    async def test_vertex_ai_pipeline_mock(self):
        """Test Vertex AI pipeline integration (mocked)."""
        from services.entity_resolution.pipeline import VertexAIPipeline
    
        with patch('google.cloud.aiplatform.PipelineJob') as mock_pipeline:
            pipeline = VertexAIPipeline()
    
            # Test pipeline execution
>           result = await pipeline.run_entity_resolution_job({
                'input_addresses': ['0xabc123' + '0' * 34],
                'confidence_threshold': 0.8
            })

tests/e2e/test_comprehensive.py:412: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
services/entity_resolution/pipeline.py:304: in run_entity_resolution_job
    PipelineJob('test-job', '/tmp/test-template.yaml')
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/pipeline_jobs.py:226: in __init__
    pipeline_json = yaml_utils.load_yaml(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/utils/yaml_utils.py:65: in load_yaml
    return _load_yaml_from_local_file(path)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file_path = '/tmp/test-template.yaml'

    def _load_yaml_from_local_file(file_path: str) -> Dict[str, Any]:
        """Loads data from a YAML local file.
    
        Args:
          file_path (str):
              Required. The local file path of the YAML document.
    
        Returns:
          A Dict object representing the YAML document.
        """
        yaml = _maybe_import_yaml()
>       with open(file_path) as f:
E       FileNotFoundError: [Errno 2] No such file or directory: '/tmp/test-template.yaml'

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/utils/yaml_utils.py:116: FileNotFoundError
_______________ TestLayer4APIVoiceOps.test_graphql_api_endpoints _______________

self = <tests.e2e.test_comprehensive.TestLayer4APIVoiceOps object at 0x139824b90>

    def test_graphql_api_endpoints(self):
        """Test GraphQL API functionality."""
        from services.graph_api.graph_api import app
    
        client = TestClient(app)
    
        # Test entity query
        query = """
        query GetEntities($limit: Int) {
            entities(limit: $limit) {
                id
                type
                addresses
                confidence
            }
        }
        """
    
        response = client.post("/graphql", json={
            "query": query,
            "variables": {"limit": 10}
        })
    
        assert response.status_code == 200
        data = response.json()
>       assert 'data' in data
E       assert 'data' in {'errors': [{'locations': [{'column': 22, 'line': 3}], 'message': "Unknown argument 'limit' on field 'Query.entities'....dress'?"}, {'locations': [{'column': 17, 'line': 7}], 'message': "Cannot query field 'confidence' on type 'Entity'."}]}

tests/e2e/test_comprehensive.py:450: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    ariadne:logger.py:21 Unknown argument 'limit' on field 'Query.entities'.

GraphQL request:3:22
2 |         query GetEntities($limit: Int) {
3 |             entities(limit: $limit) {
  |                      ^
4 |                 id
graphql.error.graphql_error.GraphQLError: Unknown argument 'limit' on field 'Query.entities'.

GraphQL request:3:22
2 |         query GetEntities($limit: Int) {
3 |             entities(limit: $limit) {
  |                      ^
4 |                 id
ERROR    ariadne:logger.py:21 Cannot query field 'addresses' on type 'Entity'. Did you mean 'address'?

GraphQL request:6:17
5 |                 type
6 |                 addresses
  |                 ^
7 |                 confidence
graphql.error.graphql_error.GraphQLError: Cannot query field 'addresses' on type 'Entity'. Did you mean 'address'?

GraphQL request:6:17
5 |                 type
6 |                 addresses
  |                 ^
7 |                 confidence
ERROR    ariadne:logger.py:21 Cannot query field 'confidence' on type 'Entity'.

GraphQL request:7:17
6 |                 addresses
7 |                 confidence
  |                 ^
8 |             }
graphql.error.graphql_error.GraphQLError: Cannot query field 'confidence' on type 'Entity'.

GraphQL request:7:17
6 |                 addresses
7 |                 confidence
  |                 ^
8 |             }
_______________ TestLayer4APIVoiceOps.test_voice_ops_integration _______________

self = <tests.e2e.test_comprehensive.TestLayer4APIVoiceOps object at 0x139816c10>

    @pytest.mark.asyncio
    async def test_voice_ops_integration(self):
        """Test voice operations (TTS/STT) with mocks."""
>       with patch('elevenlabs.generate') as mock_tts, \
             patch('speech_recognition.Recognizer') as mock_stt:

tests/e2e/test_comprehensive.py:504: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/unittest/mock.py:1437: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x14b9fda50>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'elevenlabs' from '/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/elevenlabs/__init__.py'> does not have the attribute 'generate'

../.pyenv/versions/3.11.3/lib/python3.11/unittest/mock.py:1410: AttributeError
_________ TestLayer5UXWorkflowBuilder.test_dagster_workflow_execution __________

self = <tests.e2e.test_comprehensive.TestLayer5UXWorkflowBuilder object at 0x139825810>

    def test_dagster_workflow_execution(self):
        """Test Dagster workflow execution."""
>       from services.workflow_builder.sample_signal import high_value_transfer_monitor

tests/e2e/test_comprehensive.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
___________ TestLayer5UXWorkflowBuilder.test_custom_workflow_builder ___________

self = <tests.e2e.test_comprehensive.TestLayer5UXWorkflowBuilder object at 0x1398250d0>

    def test_custom_workflow_builder(self):
        """Test dynamic workflow creation."""
>       from services.workflow_builder.sample_signal import build_custom_workflow

tests/e2e/test_comprehensive.py:545: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
__________ TestLayer6SystemIntegration.test_full_pipeline_integration __________

self = <tests.e2e.test_comprehensive.TestLayer6SystemIntegration object at 0x139826010>
mock_blockchain_data = {'block_number': 18500000, 'timestamp': 1753474843, 'transactions': [{'from': '0xabc1230000000000000000000000000000000...'from': '0xmev_bot000000000000000000000000000000', 'gas': 500000, 'gasPrice': '200000000000', 'gasUsed': 450000, ...}]}
mock_entity_resolution_data = {'entities': [{'addresses': ['0xabc1230000000000000000000000000000000000', '0xdef4560000000000000000000000000000000000...0xmev_bot000000000000000000000000000000'], 'confidence': 0.87, 'entity_id': 'ENT_002', 'entity_type': 'MEV_BOT', ...}]}

    @pytest.mark.asyncio
    async def test_full_pipeline_integration(self, mock_blockchain_data, mock_entity_resolution_data):
        """Test complete end-to-end pipeline."""
        published_signals = []
    
        # Mock all external dependencies
        with patch('services.ethereum_ingester.ethereum_ingester.Web3') as mock_web3, \
             patch('google.cloud.pubsub_v1.PublisherClient') as mock_publisher, \
             patch('neo4j.GraphDatabase.driver') as mock_neo4j, \
             patch('services.entity_resolution.pipeline.joblib.load') as mock_ml:
    
            # Setup mocks
            mock_web3_instance = Mock()
            mock_web3_instance.eth.block_number = mock_blockchain_data['block_number']
            mock_web3_instance.eth.get_block.return_value = Mock(
                number=mock_blockchain_data['block_number'],
                timestamp=mock_blockchain_data['timestamp'],
                transactions=[Mock(**tx) for tx in mock_blockchain_data['transactions']]
            )
            mock_web3.return_value = mock_web3_instance
    
            mock_pub_client = Mock()
            mock_publisher.return_value = mock_pub_client
    
            # Capture published messages
            published_messages = []
            def capture_publish(topic, message):
                published_messages.append(json.loads(message.decode('utf-8')))
                return Mock()
            mock_pub_client.publish.side_effect = capture_publish
    
            # Run ingestion
            from services.ethereum_ingester.ethereum_ingester import EthereumIngester
            ingester = EthereumIngester()
            await ingester._process_block(mock_blockchain_data['block_number'])
    
            # Verify ingestion published events
>           assert len(published_messages) > 0
E           assert 0 > 0
E            +  where 0 = len([])

tests/e2e/test_comprehensive.py:625: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:43.230925Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0x1234567890abcdef000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:43.231037Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:43.231104Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0xfedcba0987654321000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:20:43.231137Z"}
________ TestLayer6SystemIntegration.test_health_monitoring_integration ________

self = <tests.e2e.test_comprehensive.TestLayer6SystemIntegration object at 0x139826310>

    def test_health_monitoring_integration(self):
        """Test system health monitoring."""
>       from services.monitoring.health_service import HealthMonitoringService

tests/e2e/test_comprehensive.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    System Health and Monitoring Service.
    
    Provides comprehensive health checks, performance metrics, and
    operational insights for the blockchain intelligence platform.
    """
    
    import os
    import asyncio
    import json
    import time
    import logging
    import psutil
    from typing import Dict, List, Optional, Any, Tuple
    from dataclasses import dataclass, asdict
    from datetime import datetime, timedelta
    from enum import Enum
    
    import structlog
    import aiohttp
>   import aioredis
E   ModuleNotFoundError: No module named 'aioredis'

services/monitoring/health_service.py:21: ModuleNotFoundError
________________ TestSecurityCompliance.test_encryption_at_rest ________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x139827090>

    def test_encryption_at_rest(self):
        """Test data encryption capabilities."""
>       from services.access_control.audit_sink import DataEncryption
E       ImportError: cannot import name 'DataEncryption' from 'services.access_control.audit_sink' (/Users/jadenfix/eth/services/access_control/audit_sink.py)

tests/e2e/test_comprehensive.py:687: ImportError
_________________ TestSecurityCompliance.test_gdpr_compliance __________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x139827710>

    def test_gdpr_compliance(self):
        """Test GDPR data handling compliance."""
>       from services.access_control.audit_sink import GDPRCompliance
E       ImportError: cannot import name 'GDPRCompliance' from 'services.access_control.audit_sink' (/Users/jadenfix/eth/services/access_control/audit_sink.py)

tests/e2e/test_comprehensive.py:701: ImportError
_________________ TestSecurityCompliance.test_soc2_audit_trail _________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x139827d90>

    def test_soc2_audit_trail(self):
        """Test SOC 2 Type II audit trail generation."""
        from services.access_control.audit_sink import AuditLogger
    
        logger = AuditLogger()
    
        # Generate audit entries
        entries = [
            logger.log_access('user1@company.com', 'sensitive_table', 'SELECT', 'SUCCESS'),
            logger.log_access('user2@company.com', 'sensitive_table', 'UPDATE', 'DENIED'),
            logger.log_access('admin@company.com', 'system_config', 'MODIFY', 'SUCCESS')
        ]
    
        # Test audit trail completeness
        for entry in entries:
            assert 'user' in entry
            assert 'resource' in entry
            assert 'action' in entry
            assert 'result' in entry
            assert 'timestamp' in entry
>           assert 'ip_address' in entry
E           AssertionError: assert 'ip_address' in {'action': 'SELECT', 'metadata': {}, 'resource': 'sensitive_table', 'result': 'SUCCESS', ...}

tests/e2e/test_comprehensive.py:735: AssertionError
____________________________ test_neo4j_connection _____________________________

    @pytest.mark.neo4j
    def test_neo4j_connection():
        uri = os.getenv('NEO4J_URI')
        user = os.getenv('NEO4J_USER')
        password = os.getenv('NEO4J_PASSWORD')
        assert uri and user and password, "NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD must be set in the environment"
        driver = GraphDatabase.driver(uri, auth=basic_auth(user, password))
        try:
            with driver.session() as session:
>               result = session.run("RETURN 1 AS test")

tests/e2e/test_neo4j_connection.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:199: in _connect
    self._connection = self._pool.acquire(**acquire_kwargs_)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:662: in acquire
    return self._acquire(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:624: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x14f70da10>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
____________ TestIngestToBigQuery.test_ingest_synthetic_transaction ____________

self = <tests.e2e.tier0.test_t0_a_basic_ingestion.TestIngestToBigQuery object at 0x13c418d50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474851')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x14f72a8d0>
sample_chain_event = {'block_number': 18500000, 'event_type': 'transfer', 'fixture_id': 'test_sample_event', 'from_address': '0xA0b86a33E6441e8C73C3238E5A3F0B2E1f1D8E3F', ...}
clean_test_data = None

    def test_ingest_synthetic_transaction(self, gcp_env, bigquery_client, sample_chain_event, clean_test_data):
        """
        T0-A: Basic ingestion test
    
        Flow:
        1. Create test dataset and table
        2. Insert synthetic transaction
        3. Verify it appears in BigQuery
        4. Validate data structure and content
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup test infrastructure
        test_dataset = f"{gcp_env.test_prefix}_ingestion"
        test_table = "chain_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier0/test_t0_a_basic_ingestion.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x14f7288d0>
dataset_id = 'test_1753474851_ingestion', table_id = 'chain_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474851_ingestion: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
____________ TestIngestToBigQuery.test_ingest_multiple_transactions ____________

self = <tests.e2e.tier0.test_t0_a_basic_ingestion.TestIngestToBigQuery object at 0x13c419550>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474852')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x14f4eef90>
clean_test_data = None

    def test_ingest_multiple_transactions(self, gcp_env, bigquery_client, clean_test_data):
        """Test ingesting multiple transactions"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_ingestion"
        test_table = "chain_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier0/test_t0_a_basic_ingestion.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x14f4edad0>
dataset_id = 'test_1753474852_ingestion', table_id = 'chain_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474852_ingestion: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
___________ TestIngestToBigQuery.test_ingest_with_pubsub_simulation ____________

self = <tests.e2e.tier0.test_t0_a_basic_ingestion.TestIngestToBigQuery object at 0x13c419c90>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474852')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x14ef9a250>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x14f5bcf10>
clean_test_data = None

    def test_ingest_with_pubsub_simulation(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion pipeline with Pub/Sub simulation"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # Setup
        test_dataset = f"{gcp_env.test_prefix}_ingestion"
        test_table = "chain_events"
        test_topic = f"{gcp_env.test_prefix}_raw_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier0/test_t0_a_basic_ingestion.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x158169f10>
dataset_id = 'test_1753474852_ingestion', table_id = 'chain_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474852_ingestion: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
________________ TestBigQueryQueries.test_simple_bigquery_query ________________

self = <tests.e2e.tier0.test_t0_b_basic_queries.TestBigQueryQueries object at 0x13c417650>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474853')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x14f5def10>
clean_test_data = None

    def test_simple_bigquery_query(self, gcp_env, bigquery_client, clean_test_data):
        """
        T0-B: Basic BigQuery query test
    
        Flow:
        1. Insert known test data
        2. Query for that data
        3. Verify JSON structure and content
        4. Validate response format
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup test data
        test_dataset = f"{gcp_env.test_prefix}_query_test"
        test_table = "chain_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, {
            "fields": [
                {"name": "id", "type": "STRING"},
                {"name": "value", "type": "INTEGER"},
                {"name": "metadata", "type": "STRING"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        test_data = [
            {"id": "test_1", "value": 100, "metadata": '{"type": "test"}', "fixture_id": "T0_B_query"},
            {"id": "test_2", "value": 200, "metadata": '{"type": "test"}', "fixture_id": "T0_B_query"},
            {"id": "test_3", "value": 300, "metadata": '{"type": "other"}', "fixture_id": "T0_B_query"}
        ]
    
>       gcp_utils.bq_insert_rows(test_dataset, test_table, test_data)

tests/e2e/tier0/test_t0_b_basic_queries.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/helpers/gcp.py:92: in bq_insert_rows
    table = self.bq_client.get_table(table_ref)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:1208: in get_table
    api_response = self._call_api(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:859: in _call_api
    return call()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294: in retry_wrapped_func
    return retry_target(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156: in retry_target
    next_sleep = _retry_error_helper(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214: in _retry_error_helper
    raise final_exc from source_exc
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147: in retry_target
    result = target()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery._http.Connection object at 0x14f5dd890>
method = 'GET'
path = '/projects/test-project/datasets/test_1753474853_query_test/tables/chain_events'
query_params = None, data = None, content_type = None, headers = None
api_base_url = None, api_version = None, expect_json = True
_target_object = None, timeout = None, extra_api_info = None

    def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
        timeout=_DEFAULT_TIMEOUT,
        extra_api_info=None,
    ):
        """Make a request over the HTTP transport to the API.
    
        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.
    
        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.
    
        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.
    
        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.
    
        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.
    
        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.
    
        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.
    
        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.
    
        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.
    
        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.
    
        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.
    
        :type timeout: float or tuple
        :param timeout: (optional) The amount of time, in seconds, to wait
            for the server response.
    
            Can also be passed as a tuple (connect_timeout, read_timeout).
            See :meth:`requests.Session.request` documentation for details.
    
        :type extra_api_info: string
        :param extra_api_info: (optional) Extra api info to be appended to
            the X-Goog-API-Client header
    
        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )
    
        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = "application/json"
    
        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
            timeout=timeout,
            extra_api_info=extra_api_info,
        )
    
        if not 200 <= response.status_code < 300:
>           raise exceptions.from_http_response(response)
E           google.api_core.exceptions.BadRequest: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474853_query_test/tables/chain_events?prettyPrint=false: The project test-project has not enabled BigQuery.

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494: BadRequest
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474853_query_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
WARNING  tests.e2e.helpers.gcp:gcp.py:87 Failed to create table test_1753474853_query_test.chain_events: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474853_query_test/tables?prettyPrint=false: The project test-project has not enabled BigQuery.
__________________ TestBigQueryQueries.test_aggregated_query ___________________

self = <tests.e2e.tier0.test_t0_b_basic_queries.TestBigQueryQueries object at 0x13c418b10>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474854')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1583a4bd0>
clean_test_data = None

    def test_aggregated_query(self, gcp_env, bigquery_client, clean_test_data):
        """Test aggregated BigQuery queries"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_query_test"
        test_table = "transactions"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, {
            "fields": [
                {"name": "address", "type": "STRING"},
                {"name": "amount", "type": "FLOAT"},
                {"name": "category", "type": "STRING"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Insert test data
        test_data = [
            {"address": "0xA", "amount": 1.5, "category": "DeFi", "fixture_id": "T0_B_agg"},
            {"address": "0xA", "amount": 2.5, "category": "DeFi", "fixture_id": "T0_B_agg"},
            {"address": "0xB", "amount": 10.0, "category": "NFT", "fixture_id": "T0_B_agg"},
            {"address": "0xC", "amount": 0.1, "category": "DeFi", "fixture_id": "T0_B_agg"}
        ]
    
>       gcp_utils.bq_insert_rows(test_dataset, test_table, test_data)

tests/e2e/tier0/test_t0_b_basic_queries.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/helpers/gcp.py:92: in bq_insert_rows
    table = self.bq_client.get_table(table_ref)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:1208: in get_table
    api_response = self._call_api(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:859: in _call_api
    return call()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294: in retry_wrapped_func
    return retry_target(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156: in retry_target
    next_sleep = _retry_error_helper(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214: in _retry_error_helper
    raise final_exc from source_exc
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147: in retry_target
    result = target()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery._http.Connection object at 0x1583a4a50>
method = 'GET'
path = '/projects/test-project/datasets/test_1753474854_query_test/tables/transactions'
query_params = None, data = None, content_type = None, headers = None
api_base_url = None, api_version = None, expect_json = True
_target_object = None, timeout = None, extra_api_info = None

    def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
        timeout=_DEFAULT_TIMEOUT,
        extra_api_info=None,
    ):
        """Make a request over the HTTP transport to the API.
    
        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.
    
        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.
    
        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.
    
        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.
    
        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.
    
        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.
    
        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.
    
        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.
    
        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.
    
        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.
    
        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.
    
        :type timeout: float or tuple
        :param timeout: (optional) The amount of time, in seconds, to wait
            for the server response.
    
            Can also be passed as a tuple (connect_timeout, read_timeout).
            See :meth:`requests.Session.request` documentation for details.
    
        :type extra_api_info: string
        :param extra_api_info: (optional) Extra api info to be appended to
            the X-Goog-API-Client header
    
        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )
    
        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = "application/json"
    
        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
            timeout=timeout,
            extra_api_info=extra_api_info,
        )
    
        if not 200 <= response.status_code < 300:
>           raise exceptions.from_http_response(response)
E           google.api_core.exceptions.BadRequest: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474854_query_test/tables/transactions?prettyPrint=false: The project test-project has not enabled BigQuery.

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494: BadRequest
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474854_query_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
WARNING  tests.e2e.helpers.gcp:gcp.py:87 Failed to create table test_1753474854_query_test.transactions: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474854_query_test/tables?prettyPrint=false: The project test-project has not enabled BigQuery.
_________________ TestBigQueryQueries.test_query_with_filters __________________

self = <tests.e2e.tier0.test_t0_b_basic_queries.TestBigQueryQueries object at 0x13c41b650>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474855')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1581dc350>
clean_test_data = None

    def test_query_with_filters(self, gcp_env, bigquery_client, clean_test_data):
        """Test BigQuery queries with complex filters"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_query_test"
        test_table = "filtered_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, {
            "fields": [
                {"name": "timestamp", "type": "INTEGER"},
                {"name": "event_type", "type": "STRING"},
                {"name": "risk_score", "type": "FLOAT"},
                {"name": "amount", "type": "STRING"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Insert test data with various scenarios
        test_data = [
            {"timestamp": 1698000000, "event_type": "transfer", "risk_score": 0.1, "amount": "1000000", "fixture_id": "T0_B_filter"},
            {"timestamp": 1698000100, "event_type": "swap", "risk_score": 0.8, "amount": "5000000", "fixture_id": "T0_B_filter"},
            {"timestamp": 1698000200, "event_type": "transfer", "risk_score": 0.3, "amount": "500000", "fixture_id": "T0_B_filter"},
            {"timestamp": 1698000300, "event_type": "mint", "risk_score": 0.9, "amount": "10000000", "fixture_id": "T0_B_filter"}
        ]
    
>       gcp_utils.bq_insert_rows(test_dataset, test_table, test_data)

tests/e2e/tier0/test_t0_b_basic_queries.py:165: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/helpers/gcp.py:92: in bq_insert_rows
    table = self.bq_client.get_table(table_ref)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:1208: in get_table
    api_response = self._call_api(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:859: in _call_api
    return call()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294: in retry_wrapped_func
    return retry_target(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156: in retry_target
    next_sleep = _retry_error_helper(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214: in _retry_error_helper
    raise final_exc from source_exc
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147: in retry_target
    result = target()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery._http.Connection object at 0x1581dc850>
method = 'GET'
path = '/projects/test-project/datasets/test_1753474855_query_test/tables/filtered_events'
query_params = None, data = None, content_type = None, headers = None
api_base_url = None, api_version = None, expect_json = True
_target_object = None, timeout = None, extra_api_info = None

    def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
        timeout=_DEFAULT_TIMEOUT,
        extra_api_info=None,
    ):
        """Make a request over the HTTP transport to the API.
    
        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.
    
        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.
    
        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.
    
        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.
    
        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.
    
        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.
    
        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.
    
        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.
    
        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.
    
        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.
    
        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.
    
        :type timeout: float or tuple
        :param timeout: (optional) The amount of time, in seconds, to wait
            for the server response.
    
            Can also be passed as a tuple (connect_timeout, read_timeout).
            See :meth:`requests.Session.request` documentation for details.
    
        :type extra_api_info: string
        :param extra_api_info: (optional) Extra api info to be appended to
            the X-Goog-API-Client header
    
        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )
    
        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = "application/json"
    
        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
            timeout=timeout,
            extra_api_info=extra_api_info,
        )
    
        if not 200 <= response.status_code < 300:
>           raise exceptions.from_http_response(response)
E           google.api_core.exceptions.BadRequest: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474855_query_test/tables/filtered_events?prettyPrint=false: The project test-project has not enabled BigQuery.

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494: BadRequest
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474855_query_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
WARNING  tests.e2e.helpers.gcp:gcp.py:87 Failed to create table test_1753474855_query_test.filtered_events: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474855_query_test/tables?prettyPrint=false: The project test-project has not enabled BigQuery.
________________ TestNeo4jGraphQueries.test_simple_graph_query _________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x13c425a90>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x1581148d0>
clean_test_data = None

    def test_simple_graph_query(self, gcp_env, neo4j_utils, clean_test_data):
        """
        T0-C: Basic Neo4j query test
    
        Flow:
        1. Create test entities and relationships
        2. Query the graph structure
        3. Verify JSON response format
        4. Validate graph data integrity
        """
        # 1. Setup test graph data
        test_entities = [
            {
                "address": "0xT0C123",
                "type": "wallet",
                "risk_score": 0.2,
                "total_volume": 1000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "address": "0xT0C456",
                "type": "contract",
                "risk_score": 0.1,
                "total_volume": 5000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "address": "0xT0C789",
                "type": "wallet",
                "risk_score": 0.8,
                "total_volume": 500000,
                "fixture_id": "T0_C_graph"
            }
        ]
    
        test_relationships = [
            {
                "from_address": "0xT0C123",
                "to_address": "0xT0C456",
                "relationship_type": "INTERACTED_WITH",
                "transaction_count": 5,
                "total_value": 2000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "from_address": "0xT0C456",
                "to_address": "0xT0C789",
                "relationship_type": "SENT_TO",
                "transaction_count": 2,
                "total_value": 1500000,
                "fixture_id": "T0_C_graph"
            }
        ]
    
        # Load test data into Neo4j
>       neo4j_utils.load_entities(test_entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:70: AttributeError
_________________ TestNeo4jGraphQueries.test_graph_path_query __________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x13c426110>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x14f689310>
clean_test_data = None

    def test_graph_path_query(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph path queries"""
        # Setup a longer path for testing
        entities = [
            {"address": f"0xPATH{i:03d}", "type": "wallet", "risk_score": 0.1 * i, "fixture_id": "T0_C_path"}
            for i in range(4)
        ]
    
        relationships = [
            {
                "from_address": f"0xPATH{i:03d}",
                "to_address": f"0xPATH{i+1:03d}",
                "relationship_type": "SENT_TO",
                "transaction_count": 1,
                "total_value": 1000000,
                "fixture_id": "T0_C_path"
            }
            for i in range(3)
        ]
    
>       neo4j_utils.load_entities(entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:137: AttributeError
______________ TestNeo4jGraphQueries.test_graph_aggregation_query ______________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x13c426810>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x14f6888d0>
clean_test_data = None

    def test_graph_aggregation_query(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph aggregation queries"""
        # Create a hub node with multiple connections
        hub_entity = {
            "address": "0xHUB001",
            "type": "contract",
            "risk_score": 0.5,
            "fixture_id": "T0_C_agg"
        }
    
        spoke_entities = [
            {
                "address": f"0xSPOKE{i:02d}",
                "type": "wallet",
                "risk_score": 0.1 * i,
                "fixture_id": "T0_C_agg"
            }
            for i in range(5)
        ]
    
        relationships = [
            {
                "from_address": f"0xSPOKE{i:02d}",
                "to_address": "0xHUB001",
                "relationship_type": "SENT_TO",
                "transaction_count": i + 1,
                "total_value": (i + 1) * 1000000,
                "fixture_id": "T0_C_agg"
            }
            for i in range(5)
        ]
    
>       neo4j_utils.load_entities([hub_entity] + spoke_entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:190: AttributeError
________________ TestNeo4jGraphQueries.test_graph_export_format ________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x13c426f50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x14f7815d0>
clean_test_data = None

    def test_graph_export_format(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph data export in proper JSON format"""
        # Create simple test graph
        entities = [
            {"address": "0xEXPORT1", "type": "wallet", "label": "User Wallet", "fixture_id": "T0_C_export"},
            {"address": "0xEXPORT2", "type": "contract", "label": "DeFi Protocol", "fixture_id": "T0_C_export"}
        ]
    
        relationships = [
            {
                "from_address": "0xEXPORT1",
                "to_address": "0xEXPORT2",
                "relationship_type": "INTERACTED_WITH",
                "weight": 0.8,
                "fixture_id": "T0_C_export"
            }
        ]
    
>       neo4j_utils.load_entities(entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:234: AttributeError
______________ TestUIRendering.test_dashboard_loads_without_crash ______________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x13c43d7d0>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
async_http_client = <async_generator object async_http_client at 0x14f6a3300>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_dashboard_loads_without_crash(self, gcp_env, async_http_client, clean_test_data):
        """
        T0-D: Basic UI loading test
    
        Flow:
        1. Setup test data in backend
        2. Make request to dashboard endpoint
        3. Verify response is valid HTML/JSON
        4. Check for critical UI elements
        """
        # 1. Setup minimal test data for UI
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_ui_test"
        test_table = "dashboard_data"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, {
            "fields": [
                {"name": "metric_name", "type": "STRING"},
                {"name": "metric_value", "type": "FLOAT"},
                {"name": "timestamp", "type": "INTEGER"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Insert sample dashboard metrics
        dashboard_metrics = [
            {"metric_name": "total_transactions", "metric_value": 12345.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "total_volume", "metric_value": 9876543.21, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "risk_alerts", "metric_value": 23.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "active_addresses", "metric_value": 4567.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"}
        ]
    
>       gcp_utils.bq_insert_rows(test_dataset, test_table, dashboard_metrics)

tests/e2e/tier0/test_t0_d_ui_rendering.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/helpers/gcp.py:92: in bq_insert_rows
    table = self.bq_client.get_table(table_ref)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:1208: in get_table
    api_response = self._call_api(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:859: in _call_api
    return call()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294: in retry_wrapped_func
    return retry_target(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156: in retry_target
    next_sleep = _retry_error_helper(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214: in _retry_error_helper
    raise final_exc from source_exc
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147: in retry_target
    result = target()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery._http.Connection object at 0x14f5f6450>
method = 'GET'
path = '/projects/test-project/datasets/test_1753474856_ui_test/tables/dashboard_data'
query_params = None, data = None, content_type = None, headers = None
api_base_url = None, api_version = None, expect_json = True
_target_object = None, timeout = None, extra_api_info = None

    def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
        timeout=_DEFAULT_TIMEOUT,
        extra_api_info=None,
    ):
        """Make a request over the HTTP transport to the API.
    
        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.
    
        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.
    
        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.
    
        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.
    
        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.
    
        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.
    
        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.
    
        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.
    
        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.
    
        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.
    
        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.
    
        :type timeout: float or tuple
        :param timeout: (optional) The amount of time, in seconds, to wait
            for the server response.
    
            Can also be passed as a tuple (connect_timeout, read_timeout).
            See :meth:`requests.Session.request` documentation for details.
    
        :type extra_api_info: string
        :param extra_api_info: (optional) Extra api info to be appended to
            the X-Goog-API-Client header
    
        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )
    
        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = "application/json"
    
        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
            timeout=timeout,
            extra_api_info=extra_api_info,
        )
    
        if not 200 <= response.status_code < 300:
>           raise exceptions.from_http_response(response)
E           google.api_core.exceptions.BadRequest: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474856_ui_test/tables/dashboard_data?prettyPrint=false: The project test-project has not enabled BigQuery.

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494: BadRequest
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474856_ui_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
WARNING  tests.e2e.helpers.gcp:gcp.py:87 Failed to create table test_1753474856_ui_test.dashboard_data: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474856_ui_test/tables?prettyPrint=false: The project test-project has not enabled BigQuery.
______________ TestUIRendering.test_graph_visualization_endpoint _______________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x13c43de90>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
async_http_client = <async_generator object async_http_client at 0x14f652b20>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_graph_visualization_endpoint(self, gcp_env, async_http_client, clean_test_data):
        """Test graph visualization endpoint"""
        # Test graph visualization API
>       response = await async_http_client.get("/api/graph/visualization", params={
            "address": "0xTEST123",
            "depth": 2
        })
E       AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:83: AttributeError
__________________ TestUIRendering.test_health_check_endpoint __________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x13c43e550>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
async_http_client = <async_generator object async_http_client at 0x14f6534c0>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_health_check_endpoint(self, gcp_env, async_http_client, clean_test_data):
        """Test application health check"""
>       response = await async_http_client.get("/health")
E       AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:100: AttributeError
___________________ TestUIRendering.test_api_error_handling ____________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x13c43ec50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
async_http_client = <async_generator object async_http_client at 0x14f652dc0>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_api_error_handling(self, gcp_env, async_http_client, clean_test_data):
        """Test API error handling doesn't crash"""
        # Test invalid endpoints
        invalid_endpoints = [
            "/api/nonexistent",
            "/api/graph/invalid",
            "/api/dashboard/badparam"
        ]
    
        for endpoint in invalid_endpoints:
>           response = await async_http_client.get(endpoint)
E           AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:135: AttributeError
__________________ TestUIRendering.test_static_assets_loading __________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x13c43f310>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
async_http_client = <async_generator object async_http_client at 0x14f741700>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_static_assets_loading(self, gcp_env, async_http_client, clean_test_data):
        """Test that static assets load properly"""
        # Test common static asset paths
        static_paths = [
            "/static/css/main.css",
            "/static/js/app.js",
            "/assets/logo.png",
            "/favicon.ico"
        ]
    
        loaded_assets = 0
    
        for path in static_paths:
>           response = await async_http_client.get(path)
E           AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:166: AttributeError
____________ TestRealTimeIngestion.test_pubsub_to_bigquery_pipeline ____________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x13c455c50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474856')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x1582adb50>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1581f3e10>
sample_chain_event = {'block_number': 18500000, 'event_type': 'transfer', 'fixture_id': 'test_sample_event', 'from_address': '0xA0b86a33E6441e8C73C3238E5A3F0B2E1f1D8E3F', ...}
clean_test_data = None

    def test_pubsub_to_bigquery_pipeline(self, gcp_env, pubsub_publisher, bigquery_client, sample_chain_event, clean_test_data):
        """
        T1-A: End-to-end ingestion pipeline test
    
        Flow:
        1. Setup Pub/Sub topic and BigQuery destination
        2. Publish Ethereum event to Pub/Sub
        3. Simulate Dataflow processing
        4. Verify data appears correctly in BigQuery
        5. Validate data transformation and enrichment
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup pipeline infrastructure
        test_dataset = f"{gcp_env.test_prefix}_realtime_ingestion"
        test_table = "ethereum_events"
        test_topic = f"{gcp_env.test_prefix}_ethereum_raw"
        test_subscription = f"{gcp_env.test_prefix}_ethereum_processor"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x1581f3f50>
dataset_id = 'test_1753474856_realtime_ingestion', table_id = 'ethereum_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474856_realtime_ingestion: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
_______________ TestRealTimeIngestion.test_high_volume_ingestion _______________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x13c4562d0>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474857')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x14f75a2d0>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15842fc90>
clean_test_data = None

    def test_high_volume_ingestion(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion pipeline under load"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_volume_test"
        test_table = "high_volume_events"
        test_topic = f"{gcp_env.test_prefix}_volume_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15842f810>
dataset_id = 'test_1753474857_volume_test', table_id = 'high_volume_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474857_volume_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
___________ TestRealTimeIngestion.test_data_validation_and_filtering ___________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x13c456950>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474857')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x158186cd0>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x158334090>
clean_test_data = None

    def test_data_validation_and_filtering(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test data validation and filtering in ingestion pipeline"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_validation"
        test_table = "validated_events"
        test_topic = f"{gcp_env.test_prefix}_validation_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x158337cd0>
dataset_id = 'test_1753474857_validation', table_id = 'validated_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474857_validation: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
____________ TestRealTimeIngestion.test_streaming_ingestion_latency ____________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x13c456fd0>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474858')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x1580b6450>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1582cab50>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_streaming_ingestion_latency(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion latency for streaming data"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_latency"
        test_table = "latency_events"
        test_topic = f"{gcp_env.test_prefix}_latency_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15842de10>
dataset_id = 'test_1753474858_latency', table_id = 'latency_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474858_latency: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
________________ TestRealTimeIngestion.test_duplicate_detection ________________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x13c457650>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474858')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x14f783990>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x158134090>
clean_test_data = None

    def test_duplicate_detection(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test duplicate event detection and handling"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_dedup"
        test_table = "dedup_events"
        test_topic = f"{gcp_env.test_prefix}_dedup_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:349: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x158134150>
dataset_id = 'test_1753474858_dedup', table_id = 'dedup_events'
schema = [SchemaField('block_number', 'INTEGER', 'REQUIRED', None, None, (), None), SchemaField('transaction_hash', 'STRING', '...'STRING', 'REQUIRED', None, None, (), None), SchemaField('gas_used', 'INTEGER', 'REQUIRED', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474858_dedup: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
______________ TestBidirectionalSync.test_bigquery_to_neo4j_sync _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x13c4565d0>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474859')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1582cb550>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x1582ca210>
clean_test_data = None

    def test_bigquery_to_neo4j_sync(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """
        T1-B: BigQuery ‚Üí Neo4j synchronization
    
        Flow:
        1. Insert entity data into BigQuery
        2. Trigger sync process (CDC simulation)
        3. Verify entities appear in Neo4j
        4. Validate relationship creation
        5. Check data consistency
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup BigQuery entities table
        test_dataset = f"{gcp_env.test_prefix}_sync_test"
        entities_table = "entities"
        relationships_table = "relationships"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15842c550>
dataset_id = 'test_1753474859_sync_test', table_id = 'entities'
schema = [SchemaField('entity_id', 'STRING', 'REQUIRED', None, None, (), None), SchemaField('entity_type', 'STRING', 'REQUIRED'...'STRING', 'REPEATED', None, None, (), None), SchemaField('risk_score', 'FLOAT', 'NULLABLE', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474859_sync_test: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
______________ TestBidirectionalSync.test_neo4j_to_bigquery_sync _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x13c445890>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474859')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1581dfc50>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x1581dfad0>
clean_test_data = None

    def test_neo4j_to_bigquery_sync(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """
        T1-B: Neo4j ‚Üí BigQuery synchronization
    
        Flow:
        1. Create entities and relationships in Neo4j
        2. Trigger reverse sync process
        3. Verify data appears in BigQuery
        4. Check data transformation and enrichment
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup BigQuery destination tables
        test_dataset = f"{gcp_env.test_prefix}_reverse_sync"
        entities_table = "neo4j_entities"
        relationships_table = "neo4j_relationships"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x14f66ff10>
dataset_id = 'test_1753474859_reverse_sync', table_id = 'neo4j_entities'
schema = [SchemaField('entity_id', 'STRING', 'REQUIRED', None, None, (), None), SchemaField('entity_type', 'STRING', 'REQUIRED'...'STRING', 'REPEATED', None, None, (), None), SchemaField('risk_score', 'FLOAT', 'NULLABLE', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474859_reverse_sync: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
__________ TestBidirectionalSync.test_bidirectional_consistency_check __________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x13c459750>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474860')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x14f5de590>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15802e290>
clean_test_data = None

    def test_bidirectional_consistency_check(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test data consistency between BigQuery and Neo4j after bidirectional sync"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # Setup test environment
        test_dataset = f"{gcp_env.test_prefix}_consistency"
        entities_table = "entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x14f66ec90>
dataset_id = 'test_1753474860_consistency', table_id = 'entities'
schema = [SchemaField('entity_id', 'STRING', 'REQUIRED', None, None, (), None), SchemaField('entity_type', 'STRING', 'REQUIRED'...'STRING', 'REPEATED', None, None, (), None), SchemaField('risk_score', 'FLOAT', 'NULLABLE', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474860_consistency: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
______________ TestBidirectionalSync.test_real_time_sync_latency _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x13c459a50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474860')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1581cdbd0>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x1581ce810>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_real_time_sync_latency(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test latency of real-time bidirectional sync"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_sync_latency"
        entities_table = "real_time_entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
>       gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:476: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x1581cee90>
dataset_id = 'test_1753474860_sync_latency', table_id = 'real_time_entities'
schema = [SchemaField('entity_id', 'STRING', 'REQUIRED', None, None, (), None), SchemaField('entity_type', 'STRING', 'REQUIRED'...'STRING', 'REPEATED', None, None, (), None), SchemaField('risk_score', 'FLOAT', 'NULLABLE', None, None, (), None), ...]

    def bq_create_table(self, dataset_id: str, table_id: str, schema: Dict) -> None:
        """Create BigQuery table with schema"""
        client = self._get_bq_client()
        table_ref = client.dataset(dataset_id).table(table_id)
    
        # Convert schema dict to BigQuery schema
        bq_schema = []
>       for field in schema.get("fields", []):
E       AttributeError: 'list' object has no attribute 'get'

tests/e2e/helpers/gcp.py:73: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474860_sync_latency: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
_____________ TestBidirectionalSync.test_sync_conflict_resolution ______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x13c459d50>
gcp_env = GCPTestEnvironment(project_id='test-project', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753474861')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x1580994d0>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x158099510>
clean_test_data = None

    def test_sync_conflict_resolution(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test conflict resolution when same entity is modified in both stores"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_conflicts"
        entities_table = "conflict_entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, {
            "fields": [
                {"name": "address", "type": "STRING"},
                {"name": "entity_type", "type": "STRING"},
                {"name": "risk_score", "type": "FLOAT"},
                {"name": "total_volume", "type": "FLOAT"},
                {"name": "last_modified", "type": "INTEGER"},
                {"name": "modified_in", "type": "STRING"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Create initial entity
        base_entity = {
            "address": "0xCONFLICT001",
            "entity_type": "wallet",
            "risk_score": 0.5,
            "total_volume": 1000000.0,
            "last_modified": int(time.time()),
            "modified_in": "initial",
            "fixture_id": "T1_B_conflict"
        }
    
        # Insert to both stores
>       gcp_utils.bq_insert_rows(test_dataset, entities_table, [base_entity])

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:570: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/helpers/gcp.py:92: in bq_insert_rows
    table = self.bq_client.get_table(table_ref)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:1208: in get_table
    api_response = self._call_api(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/bigquery/client.py:859: in _call_api
    return call()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294: in retry_wrapped_func
    return retry_target(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156: in retry_target
    next_sleep = _retry_error_helper(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214: in _retry_error_helper
    raise final_exc from source_exc
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147: in retry_target
    result = target()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery._http.Connection object at 0x1581cf410>
method = 'GET'
path = '/projects/test-project/datasets/test_1753474861_conflicts/tables/conflict_entities'
query_params = None, data = None, content_type = None, headers = None
api_base_url = None, api_version = None, expect_json = True
_target_object = None, timeout = None, extra_api_info = None

    def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
        timeout=_DEFAULT_TIMEOUT,
        extra_api_info=None,
    ):
        """Make a request over the HTTP transport to the API.
    
        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.
    
        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.
    
        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.
    
        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.
    
        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.
    
        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.
    
        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.
    
        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.
    
        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.
    
        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.
    
        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.
    
        :type timeout: float or tuple
        :param timeout: (optional) The amount of time, in seconds, to wait
            for the server response.
    
            Can also be passed as a tuple (connect_timeout, read_timeout).
            See :meth:`requests.Session.request` documentation for details.
    
        :type extra_api_info: string
        :param extra_api_info: (optional) Extra api info to be appended to
            the X-Goog-API-Client header
    
        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )
    
        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = "application/json"
    
        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
            timeout=timeout,
            extra_api_info=extra_api_info,
        )
    
        if not 200 <= response.status_code < 300:
>           raise exceptions.from_http_response(response)
E           google.api_core.exceptions.BadRequest: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474861_conflicts/tables/conflict_entities?prettyPrint=false: The project test-project has not enabled BigQuery.

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494: BadRequest
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.helpers.gcp:gcp.py:64 Failed to create dataset test_1753474861_conflicts: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets?prettyPrint=false: The project test-project has not enabled BigQuery.
WARNING  tests.e2e.helpers.gcp:gcp.py:87 Failed to create table test_1753474861_conflicts.conflict_entities: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/test-project/datasets/test_1753474861_conflicts/tables?prettyPrint=false: The project test-project has not enabled BigQuery.
_______ TestRealServiceIntegration.test_neo4j_graph_database_integration _______

self = <tests.e2e.tier2.test_t2_a_real_service_integration.TestRealServiceIntegration object at 0x13c480190>

    def test_neo4j_graph_database_integration(self):
        """Test Neo4j Aura database connectivity and operations"""
        neo4j_uri = os.getenv('NEO4J_URI')
        neo4j_user = os.getenv('NEO4J_USER')
        neo4j_password = os.getenv('NEO4J_PASSWORD')
    
        driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
    
        try:
            # Test connection
            with driver.session() as session:
>               result = session.run("RETURN 'Hello Neo4j' as message")

tests/e2e/tier2/test_t2_a_real_service_integration.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x1582f9990>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
_____________ TestRealServiceIntegration.test_websocket_endpoints ______________

self = <tests.e2e.tier2.test_t2_a_real_service_integration.TestRealServiceIntegration object at 0x13c481d90>

    @pytest.mark.asyncio
    async def test_websocket_endpoints(self):
        """Test WebSocket endpoints for real-time data"""
        ws_endpoint = os.getenv('NEXT_PUBLIC_WEBSOCKET_ENDPOINT', 'ws://localhost:4000/subscriptions')
    
        try:
>           async with websockets.connect(ws_endpoint, timeout=10) as websocket:

tests/e2e/tier2/test_t2_a_real_service_integration.py:345: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:587: in __aenter__
    return await self
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:541: in __await_impl__
    self.connection = await self.create_connection()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <websockets.asyncio.client.connect object at 0x158093510>

    async def create_connection(self) -> ClientConnection:
        """Create TCP or Unix connection."""
        loop = asyncio.get_running_loop()
        kwargs = self.connection_kwargs.copy()
    
        ws_uri = parse_uri(self.uri)
    
        proxy = self.proxy
        if kwargs.get("unix", False):
            proxy = None
        if kwargs.get("sock") is not None:
            proxy = None
        if proxy is True:
            proxy = get_proxy(ws_uri)
    
        def factory() -> ClientConnection:
            return self.protocol_factory(ws_uri)
    
        if ws_uri.secure:
            kwargs.setdefault("ssl", True)
            kwargs.setdefault("server_hostname", ws_uri.host)
            if kwargs.get("ssl") is None:
                raise ValueError("ssl=None is incompatible with a wss:// URI")
        else:
            if kwargs.get("ssl") is not None:
                raise ValueError("ssl argument is incompatible with a ws:// URI")
    
        if kwargs.pop("unix", False):
            _, connection = await loop.create_unix_connection(factory, **kwargs)
        elif proxy is not None:
            proxy_parsed = parse_proxy(proxy)
            if proxy_parsed.scheme[:5] == "socks":
                # Connect to the server through the proxy.
                sock = await connect_socks_proxy(
                    proxy_parsed,
                    ws_uri,
                    local_addr=kwargs.pop("local_addr", None),
                )
                # Initialize WebSocket connection via the proxy.
                _, connection = await loop.create_connection(
                    factory,
                    sock=sock,
                    **kwargs,
                )
            elif proxy_parsed.scheme[:4] == "http":
                # Split keyword arguments between the proxy and the server.
                all_kwargs, proxy_kwargs, kwargs = kwargs, {}, {}
                for key, value in all_kwargs.items():
                    if key.startswith("ssl") or key == "server_hostname":
                        kwargs[key] = value
                    elif key.startswith("proxy_"):
                        proxy_kwargs[key[6:]] = value
                    else:
                        proxy_kwargs[key] = value
                # Validate the proxy_ssl argument.
                if proxy_parsed.scheme == "https":
                    proxy_kwargs.setdefault("ssl", True)
                    if proxy_kwargs.get("ssl") is None:
                        raise ValueError(
                            "proxy_ssl=None is incompatible with an https:// proxy"
                        )
                else:
                    if proxy_kwargs.get("ssl") is not None:
                        raise ValueError(
                            "proxy_ssl argument is incompatible with an http:// proxy"
                        )
                # Connect to the server through the proxy.
                transport = await connect_http_proxy(
                    proxy_parsed,
                    ws_uri,
                    user_agent_header=self.user_agent_header,
                    **proxy_kwargs,
                )
                # Initialize WebSocket connection via the proxy.
                connection = factory()
                transport.set_protocol(connection)
                ssl = kwargs.pop("ssl", None)
                if ssl is True:
                    ssl = ssl_module.create_default_context()
                if ssl is not None:
                    new_transport = await loop.start_tls(
                        transport, connection, ssl, **kwargs
                    )
                    assert new_transport is not None  # help mypy
                    transport = new_transport
                connection.connection_made(transport)
            else:
                raise AssertionError("unsupported proxy")
        else:
            # Connect to the server directly.
            if kwargs.get("sock") is None:
                kwargs.setdefault("host", ws_uri.host)
                kwargs.setdefault("port", ws_uri.port)
            # Initialize WebSocket connection.
>           _, connection = await loop.create_connection(factory, **kwargs)
E           TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'timeout'

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:467: TypeError
________ TestV3PatchesIntegration.test_patch_1_bidirectional_graph_sync ________

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c488a50>

    def test_patch_1_bidirectional_graph_sync(self):
        """
        Patch 1: Bidirectional Graph Sync (BigQuery ‚Üî Neo4j)
        Test CDC and real-time synchronization
        """
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
    
        # Initialize clients
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        test_id = f"patch1_test_{int(time.time())}"
    
        try:
            # Skip BigQuery operations if permissions are restricted
            # Instead, simulate the data flow with Neo4j as the primary store
    
            entity_data = {
                "entity_id": test_id,
                "address": f"0x{test_id}",
                "entity_type": "wallet",
                "risk_score": 0.75,
                "total_volume": 5000000.0,
                "labels": json.dumps(["high_risk", "whale"]),
                "last_updated": time.time(),
                "patch_test": "patch_1_bidirectional_sync"
            }
    
            # 1. Store entity in Neo4j (simulating initial data ingestion)
            with neo4j_driver.session() as session:
                create_query = """
                CREATE (e:Entity {
                    entity_id: $entity_id,
                    address: $address,
                    entity_type: $entity_type,
                    risk_score: $risk_score,
                    total_volume: $total_volume,
                    labels: $labels,
                    last_updated: $last_updated,
                    patch_test: $patch_test,
                    source: 'initial_ingestion',
                    sync_version: 0
                })
                RETURN e.entity_id as created_id
                """
    
>               result = session.run(create_query, entity_data)

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a543550>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError

During handling of the above exception, another exception occurred:

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c488a50>

    def test_patch_1_bidirectional_graph_sync(self):
        """
        Patch 1: Bidirectional Graph Sync (BigQuery ‚Üî Neo4j)
        Test CDC and real-time synchronization
        """
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
    
        # Initialize clients
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        test_id = f"patch1_test_{int(time.time())}"
    
        try:
            # Skip BigQuery operations if permissions are restricted
            # Instead, simulate the data flow with Neo4j as the primary store
    
            entity_data = {
                "entity_id": test_id,
                "address": f"0x{test_id}",
                "entity_type": "wallet",
                "risk_score": 0.75,
                "total_volume": 5000000.0,
                "labels": json.dumps(["high_risk", "whale"]),
                "last_updated": time.time(),
                "patch_test": "patch_1_bidirectional_sync"
            }
    
            # 1. Store entity in Neo4j (simulating initial data ingestion)
            with neo4j_driver.session() as session:
                create_query = """
                CREATE (e:Entity {
                    entity_id: $entity_id,
                    address: $address,
                    entity_type: $entity_type,
                    risk_score: $risk_score,
                    total_volume: $total_volume,
                    labels: $labels,
                    last_updated: $last_updated,
                    patch_test: $patch_test,
                    source: 'initial_ingestion',
                    sync_version: 0
                })
                RETURN e.entity_id as created_id
                """
    
                result = session.run(create_query, entity_data)
                created = result.single()
                assert created["created_id"] == test_id
    
            # 2. Simulate graph analysis updates (core of Patch 1)
            with neo4j_driver.session() as session:
                update_query = """
                MATCH (e:Entity {entity_id: $entity_id, patch_test: $patch_test})
                SET e.risk_score = 0.95,
                    e.graph_analysis_score = 0.88,
                    e.updated_in_neo4j = true,
                    e.neo4j_update_time = timestamp(),
                    e.sync_version = e.sync_version + 1
                RETURN e.risk_score as new_risk_score, e.graph_analysis_score as graph_score
                """
    
                result = session.run(update_query, {
                    "entity_id": test_id,
                    "patch_test": entity_data["patch_test"]
                })
    
                updated = result.single()
                assert updated["new_risk_score"] == 0.95
                assert updated["graph_score"] == 0.88
    
            # 3. Verify bidirectional sync capability (entity can be retrieved with updates)
            with neo4j_driver.session() as session:
                verify_query = """
                MATCH (e:Entity {entity_id: $entity_id, patch_test: $patch_test})
                RETURN e.risk_score as risk_score,
                       e.graph_analysis_score as graph_analysis_score,
                       e.updated_in_neo4j as was_updated,
                       e.sync_version as version
                """
    
                result = session.run(verify_query, {
                    "entity_id": test_id,
                    "patch_test": entity_data["patch_test"]
                })
    
                verification = result.single()
                assert verification["risk_score"] == 0.95
                assert verification["graph_analysis_score"] == 0.88
                assert verification["was_updated"] is True
                assert verification["version"] >= 1
    
            print("‚úÖ Patch 1: Bidirectional Graph Sync test passed (Neo4j-centric)")
    
        finally:
            # Cleanup
            with neo4j_driver.session() as session:
>               session.run("""
                MATCH (e:Entity {patch_test: 'patch_1_bidirectional_sync'})
                DELETE e
                """)

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x1580911d0>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
__________ TestV3PatchesIntegration.test_patch_2_zk_attested_signals ___________

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c489090>

    def test_patch_2_zk_attested_signals(self):
        """
        Patch 2: ZK-Attested Signals
        Test cryptographic proof generation and verification
        """
        # For this test, we'll simulate the ZK proof workflow
        # In a real implementation, this would use Circom circuits
    
        test_signal = {
            "signal_id": f"zk_test_{int(time.time())}",
            "wallet_address": "0xtest123",
            "risk_score": 0.92,
            "evidence": ["large_transaction", "new_address", "multiple_exchanges"],
            "timestamp": time.time(),
            "patch_test": "patch_2_zk_attestation"
        }
    
        # Simulate proof generation (would use snarkJS in real implementation)
        import hashlib
    
        signal_data = json.dumps(test_signal, sort_keys=True)
        signal_hash = hashlib.sha256(signal_data.encode()).hexdigest()
    
        # Simulate ZK proof structure
        simulated_proof = {
            "proof": {
                "pi_a": ["0x" + "a" * 64, "0x" + "b" * 64, "0x1"],
                "pi_b": [["0x" + "c" * 64, "0x" + "d" * 64], ["0x" + "e" * 64, "0x" + "f" * 64], ["0x1", "0x0"]],
                "pi_c": ["0x" + "g" * 64, "0x" + "h" * 64, "0x1"]
            },
            "publicSignals": [signal_hash[:16]]  # Truncated for example
        }
    
        # Test proof verification API endpoint
        verification_payload = {
            "signal": test_signal,
            "proof": simulated_proof,
            "signal_hash": signal_hash
        }
    
        # Store the attested signal in Neo4j (since BigQuery has permission issues)
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        try:
            with neo4j_driver.session() as session:
                # Store attested signal in Neo4j
                store_signal_query = """
                CREATE (s:AttestedSignal {
                    signal_id: $signal_id,
                    wallet_address: $wallet_address,
                    risk_score: $risk_score,
                    signal_hash: $signal_hash,
                    proof_verified: true,
                    evidence: $evidence,
                    timestamp: $timestamp,
                    patch_test: $patch_test
                })
                RETURN s.signal_id as stored_id
                """
    
>               result = session.run(store_signal_query, {
                    "signal_id": test_signal["signal_id"],
                    "wallet_address": test_signal["wallet_address"],
                    "risk_score": test_signal["risk_score"],
                    "signal_hash": signal_hash,
                    "evidence": json.dumps(test_signal["evidence"]),
                    "timestamp": test_signal["timestamp"],
                    "patch_test": test_signal["patch_test"]
                })

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15aa30cd0>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError

During handling of the above exception, another exception occurred:

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c489090>

    def test_patch_2_zk_attested_signals(self):
        """
        Patch 2: ZK-Attested Signals
        Test cryptographic proof generation and verification
        """
        # For this test, we'll simulate the ZK proof workflow
        # In a real implementation, this would use Circom circuits
    
        test_signal = {
            "signal_id": f"zk_test_{int(time.time())}",
            "wallet_address": "0xtest123",
            "risk_score": 0.92,
            "evidence": ["large_transaction", "new_address", "multiple_exchanges"],
            "timestamp": time.time(),
            "patch_test": "patch_2_zk_attestation"
        }
    
        # Simulate proof generation (would use snarkJS in real implementation)
        import hashlib
    
        signal_data = json.dumps(test_signal, sort_keys=True)
        signal_hash = hashlib.sha256(signal_data.encode()).hexdigest()
    
        # Simulate ZK proof structure
        simulated_proof = {
            "proof": {
                "pi_a": ["0x" + "a" * 64, "0x" + "b" * 64, "0x1"],
                "pi_b": [["0x" + "c" * 64, "0x" + "d" * 64], ["0x" + "e" * 64, "0x" + "f" * 64], ["0x1", "0x0"]],
                "pi_c": ["0x" + "g" * 64, "0x" + "h" * 64, "0x1"]
            },
            "publicSignals": [signal_hash[:16]]  # Truncated for example
        }
    
        # Test proof verification API endpoint
        verification_payload = {
            "signal": test_signal,
            "proof": simulated_proof,
            "signal_hash": signal_hash
        }
    
        # Store the attested signal in Neo4j (since BigQuery has permission issues)
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        try:
            with neo4j_driver.session() as session:
                # Store attested signal in Neo4j
                store_signal_query = """
                CREATE (s:AttestedSignal {
                    signal_id: $signal_id,
                    wallet_address: $wallet_address,
                    risk_score: $risk_score,
                    signal_hash: $signal_hash,
                    proof_verified: true,
                    evidence: $evidence,
                    timestamp: $timestamp,
                    patch_test: $patch_test
                })
                RETURN s.signal_id as stored_id
                """
    
                result = session.run(store_signal_query, {
                    "signal_id": test_signal["signal_id"],
                    "wallet_address": test_signal["wallet_address"],
                    "risk_score": test_signal["risk_score"],
                    "signal_hash": signal_hash,
                    "evidence": json.dumps(test_signal["evidence"]),
                    "timestamp": test_signal["timestamp"],
                    "patch_test": test_signal["patch_test"]
                })
    
                stored = result.single()
                assert stored["stored_id"] == test_signal["signal_id"]
    
                # Verify the attested signal was stored
                verify_query = """
                MATCH (s:AttestedSignal {patch_test: 'patch_2_zk_attestation'})
                RETURN s.signal_id as signal_id,
                       s.proof_verified as proof_verified,
                       s.signal_hash as signal_hash
                """
    
                result = session.run(verify_query)
                results = list(result)
    
                assert len(results) == 1
                assert results[0]["proof_verified"] is True
                assert results[0]["signal_hash"] == signal_hash
    
            print("‚úÖ Patch 2: ZK-Attested Signals test passed (Neo4j-based)")
    
        finally:
            # Cleanup
            with neo4j_driver.session() as session:
>               session.run("""
                MATCH (s:AttestedSignal {patch_test: 'patch_2_zk_attestation'})
                DELETE s
                """)

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:238: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a5d2bd0>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
_______ TestV3PatchesIntegration.test_patch_4_autonomous_action_executor _______

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c489d90>

    def test_patch_4_autonomous_action_executor(self):
        """
        Patch 4: Autonomous Action Executor
        Test automated response system with YAML playbooks
        """
        # Create test playbook configuration
        test_playbook = {
            "name": "high_risk_wallet_response",
            "trigger": {
                "risk_score_threshold": 0.9,
                "confidence_threshold": 0.8
            },
            "actions": [
                {
                    "type": "freeze_position",
                    "params": {
                        "duration": "24h",
                        "notify_compliance": True
                    }
                },
                {
                    "type": "hedge_exposure",
                    "params": {
                        "percentage": 50,
                        "instruments": ["USDC", "ETH"]
                    }
                },
                {
                    "type": "alert_notification",
                    "params": {
                        "channels": ["slack", "email"],
                        "priority": "high"
                    }
                }
            ]
        }
    
        # Simulate high-risk signal that triggers action executor
        trigger_signal = {
            "signal_id": f"action_test_{int(time.time())}",
            "wallet_address": "0xhighrisk789",
            "risk_score": 0.95,
            "confidence": 0.92,
            "signal_type": "money_laundering_detected",
            "evidence": ["mixer_interaction", "rapid_transactions", "suspicious_amounts"],
            "timestamp": time.time()
        }
    
        # Test action execution simulation
        executed_actions = []
    
        for action in test_playbook["actions"]:
            if action["type"] == "freeze_position":
                # Simulate position freeze
                freeze_result = {
                    "action_type": "freeze_position",
                    "wallet_address": trigger_signal["wallet_address"],
                    "duration": action["params"]["duration"],
                    "status": "executed",
                    "timestamp": time.time(),
                    "dry_run": True  # Safety flag
                }
                executed_actions.append(freeze_result)
    
            elif action["type"] == "hedge_exposure":
                # Simulate hedge execution
                hedge_result = {
                    "action_type": "hedge_exposure",
                    "wallet_address": trigger_signal["wallet_address"],
                    "hedge_percentage": action["params"]["percentage"],
                    "instruments": action["params"]["instruments"],
                    "status": "executed",
                    "timestamp": time.time(),
                    "dry_run": True
                }
                executed_actions.append(hedge_result)
    
            elif action["type"] == "alert_notification":
                # Simulate notification sending
                alert_result = {
                    "action_type": "alert_notification",
                    "wallet_address": trigger_signal["wallet_address"],
                    "channels": action["params"]["channels"],
                    "priority": action["params"]["priority"],
                    "status": "sent",
                    "timestamp": time.time()
                }
                executed_actions.append(alert_result)
    
        # Store action execution results
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
        bq_client = bigquery.Client(project=project_id)
    
        table_id = f"{dataset_id}.action_executions"
        schema = [
            bigquery.SchemaField("signal_id", "STRING"),
            bigquery.SchemaField("wallet_address", "STRING"),
            bigquery.SchemaField("action_type", "STRING"),
            bigquery.SchemaField("status", "STRING"),
            bigquery.SchemaField("execution_details", "STRING"),
            bigquery.SchemaField("executed_at", "FLOAT"),
            bigquery.SchemaField("dry_run", "BOOLEAN"),
            bigquery.SchemaField("patch_test", "STRING"),
        ]
    
        table = bigquery.Table(f"{project_id}.{table_id}", schema=schema)
        bq_client.create_table(table, exists_ok=True)
    
        # Insert execution records
        execution_records = []
        for action in executed_actions:
            record = {
                "signal_id": trigger_signal["signal_id"],
                "wallet_address": trigger_signal["wallet_address"],
                "action_type": action["action_type"],
                "status": action["status"],
                "execution_details": json.dumps(action),
                "executed_at": action["timestamp"],
                "dry_run": action.get("dry_run", False),
                "patch_test": "patch_4_action_executor"
            }
            execution_records.append(record)
    
        errors = bq_client.insert_rows_json(
            bq_client.get_table(table_id),
            execution_records
        )
        assert len(errors) == 0
    
        # Verify actions were executed
        query = f"""
        SELECT COUNT(*) as action_count,
               COUNTIF(status = 'executed' OR status = 'sent') as successful_actions
        FROM `{project_id}.{table_id}`
        WHERE patch_test = 'patch_4_action_executor'
        """
    
        query_job = bq_client.query(query)
        results = list(query_job.result())
    
        assert len(results) == 1
>       assert results[0].action_count == 3  # All three actions
E       AssertionError: assert 36 == 3
E        +  where 36 = Row((36, 36), {'action_count': 0, 'successful_actions': 1}).action_count

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:485: AssertionError
____________ TestV3PatchesIntegration.test_patch_5_voice_ops_polish ____________

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c48a3d0>

    @pytest.mark.asyncio
    async def test_patch_5_voice_ops_polish(self):
        """
        Patch 5: Voice Operations Polish
        Test ElevenLabs TTS and WebSocket integration
        """
        api_key = os.getenv('ELEVENLABS_API_KEY')
        voice_id = os.getenv('ELEVENLABS_VOICE_ID')
    
        # Test voice alert generation
        alert_message = "High risk wallet detected. Address 0xtest789 flagged for suspicious activity. Immediate attention required."
    
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Generate TTS for alert
            tts_payload = {
                "text": alert_message,
                "model_id": "eleven_monolingual_v1",
                "voice_settings": {
                    "stability": 0.7,
                    "similarity_boost": 0.8,
                    "style": 0.2,
                    "use_speaker_boost": True
                }
            }
    
            tts_response = await client.post(
                f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}",
                headers={
                    "xi-api-key": api_key,
                    "Content-Type": "application/json"
                },
                json=tts_payload
            )
    
            assert tts_response.status_code == 200
            assert len(tts_response.content) > 1000  # Audio file should be substantial
    
            # Test voice notification storage in Neo4j (since BigQuery has permission issues)
            voice_notification = {
                "type": "voice_alert",
                "message": alert_message,
                "priority": "high",
                "wallet_address": "0xtest789",
                "audio_length": len(tts_response.content),
                "generated_at": time.time()
            }
    
            # Store voice notification record in Neo4j
            neo4j_driver = GraphDatabase.driver(
                os.getenv('NEO4J_URI'),
                auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
            )
    
            try:
                with neo4j_driver.session() as session:
                    store_notification_query = """
                    CREATE (v:VoiceNotification {
                        notification_id: $notification_id,
                        message_text: $message_text,
                        wallet_address: $wallet_address,
                        priority: $priority,
                        audio_length_bytes: $audio_length_bytes,
                        generated_at: $generated_at,
                        patch_test: $patch_test
                    })
                    RETURN v.notification_id as stored_id
                    """
    
>                   result = session.run(store_notification_query, {
                        "notification_id": f"voice_{int(time.time())}",
                        "message_text": alert_message,
                        "wallet_address": "0xtest789",
                        "priority": "high",
                        "audio_length_bytes": len(tts_response.content),
                        "generated_at": time.time(),
                        "patch_test": "patch_5_voice_polish"
                    })

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:558: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a573550>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError

During handling of the above exception, another exception occurred:

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c48a3d0>

    @pytest.mark.asyncio
    async def test_patch_5_voice_ops_polish(self):
        """
        Patch 5: Voice Operations Polish
        Test ElevenLabs TTS and WebSocket integration
        """
        api_key = os.getenv('ELEVENLABS_API_KEY')
        voice_id = os.getenv('ELEVENLABS_VOICE_ID')
    
        # Test voice alert generation
        alert_message = "High risk wallet detected. Address 0xtest789 flagged for suspicious activity. Immediate attention required."
    
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Generate TTS for alert
            tts_payload = {
                "text": alert_message,
                "model_id": "eleven_monolingual_v1",
                "voice_settings": {
                    "stability": 0.7,
                    "similarity_boost": 0.8,
                    "style": 0.2,
                    "use_speaker_boost": True
                }
            }
    
            tts_response = await client.post(
                f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}",
                headers={
                    "xi-api-key": api_key,
                    "Content-Type": "application/json"
                },
                json=tts_payload
            )
    
            assert tts_response.status_code == 200
            assert len(tts_response.content) > 1000  # Audio file should be substantial
    
            # Test voice notification storage in Neo4j (since BigQuery has permission issues)
            voice_notification = {
                "type": "voice_alert",
                "message": alert_message,
                "priority": "high",
                "wallet_address": "0xtest789",
                "audio_length": len(tts_response.content),
                "generated_at": time.time()
            }
    
            # Store voice notification record in Neo4j
            neo4j_driver = GraphDatabase.driver(
                os.getenv('NEO4J_URI'),
                auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
            )
    
            try:
                with neo4j_driver.session() as session:
                    store_notification_query = """
                    CREATE (v:VoiceNotification {
                        notification_id: $notification_id,
                        message_text: $message_text,
                        wallet_address: $wallet_address,
                        priority: $priority,
                        audio_length_bytes: $audio_length_bytes,
                        generated_at: $generated_at,
                        patch_test: $patch_test
                    })
                    RETURN v.notification_id as stored_id
                    """
    
                    result = session.run(store_notification_query, {
                        "notification_id": f"voice_{int(time.time())}",
                        "message_text": alert_message,
                        "wallet_address": "0xtest789",
                        "priority": "high",
                        "audio_length_bytes": len(tts_response.content),
                        "generated_at": time.time(),
                        "patch_test": "patch_5_voice_polish"
                    })
    
                    stored = result.single()
                    assert stored["stored_id"] is not None
    
                    # Verify voice notification was recorded
                    verify_query = """
                    MATCH (v:VoiceNotification {patch_test: 'patch_5_voice_polish'})
                    RETURN v.notification_id as notification_id,
                           v.audio_length_bytes as audio_length_bytes,
                           v.priority as priority
                    ORDER BY v.generated_at DESC
                    LIMIT 1
                    """
    
                    result = session.run(verify_query)
                    results = list(result)
    
                    assert len(results) == 1
                    assert results[0]["audio_length_bytes"] > 1000
                    assert results[0]["priority"] == "high"
    
                print("‚úÖ Patch 5: Voice Operations Polish test passed (Neo4j-based)")
    
            finally:
                # Cleanup
                with neo4j_driver.session() as session:
>                   session.run("""
                    MATCH (v:VoiceNotification {patch_test: 'patch_5_voice_polish'})
                    DELETE v
                    """)

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a571650>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
___________ TestV3PatchesIntegration.test_v3_integration_end_to_end ____________

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c48aa50>

    def test_v3_integration_end_to_end(self):
        """
        Complete V3 integration test: All patches working together
        """
        test_scenario = {
            "wallet_address": "0xe2etest123",
            "initial_risk_score": 0.3,
            "suspicious_transaction": {
                "hash": "0xe2etx456",
                "value": 10000000,  # 10M wei
                "to_address": "0xsuspicious789",
                "flags": ["large_amount", "new_counterparty"]
            }
        }
    
        # 1. Ingest transaction (triggers analysis)
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
        bq_client = bigquery.Client(project=project_id)
    
        # Store transaction
        tx_table_id = f"{dataset_id}.transactions"
        tx_schema = [
            bigquery.SchemaField("tx_hash", "STRING"),
            bigquery.SchemaField("from_address", "STRING"),
            bigquery.SchemaField("to_address", "STRING"),
            bigquery.SchemaField("value", "INTEGER"),
            bigquery.SchemaField("timestamp", "FLOAT"),
            bigquery.SchemaField("risk_flags", "STRING"),
            bigquery.SchemaField("scenario_test", "STRING"),
        ]
    
        tx_table = bigquery.Table(f"{project_id}.{tx_table_id}", schema=tx_schema)
        bq_client.create_table(tx_table, exists_ok=True)
    
        tx_data = {
            "tx_hash": test_scenario["suspicious_transaction"]["hash"],
            "from_address": test_scenario["wallet_address"],
            "to_address": test_scenario["suspicious_transaction"]["to_address"],
            "value": test_scenario["suspicious_transaction"]["value"],
            "timestamp": time.time(),
            "risk_flags": json.dumps(test_scenario["suspicious_transaction"]["flags"]),
            "scenario_test": "v3_integration_e2e"
        }
    
        errors = bq_client.insert_rows_json(bq_client.get_table(tx_table_id), [tx_data])
        assert len(errors) == 0
    
        # 2. Risk analysis updates wallet score (Patch 1: Bidirectional sync)
        updated_risk_score = 0.94
    
        # Update in Neo4j (simulating graph analysis)
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        try:
            with neo4j_driver.session() as session:
>               session.run("""
                MERGE (w:Wallet {address: $address})
                SET w.risk_score = $risk_score,
                    w.last_analysis = timestamp(),
                    w.scenario_test = $scenario_test
                """, {
                    "address": test_scenario["wallet_address"],
                    "risk_score": updated_risk_score,
                    "scenario_test": "v3_integration_e2e"
                })

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:658: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a5e6010>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError

During handling of the above exception, another exception occurred:

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x13c48aa50>

    def test_v3_integration_end_to_end(self):
        """
        Complete V3 integration test: All patches working together
        """
        test_scenario = {
            "wallet_address": "0xe2etest123",
            "initial_risk_score": 0.3,
            "suspicious_transaction": {
                "hash": "0xe2etx456",
                "value": 10000000,  # 10M wei
                "to_address": "0xsuspicious789",
                "flags": ["large_amount", "new_counterparty"]
            }
        }
    
        # 1. Ingest transaction (triggers analysis)
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
        bq_client = bigquery.Client(project=project_id)
    
        # Store transaction
        tx_table_id = f"{dataset_id}.transactions"
        tx_schema = [
            bigquery.SchemaField("tx_hash", "STRING"),
            bigquery.SchemaField("from_address", "STRING"),
            bigquery.SchemaField("to_address", "STRING"),
            bigquery.SchemaField("value", "INTEGER"),
            bigquery.SchemaField("timestamp", "FLOAT"),
            bigquery.SchemaField("risk_flags", "STRING"),
            bigquery.SchemaField("scenario_test", "STRING"),
        ]
    
        tx_table = bigquery.Table(f"{project_id}.{tx_table_id}", schema=tx_schema)
        bq_client.create_table(tx_table, exists_ok=True)
    
        tx_data = {
            "tx_hash": test_scenario["suspicious_transaction"]["hash"],
            "from_address": test_scenario["wallet_address"],
            "to_address": test_scenario["suspicious_transaction"]["to_address"],
            "value": test_scenario["suspicious_transaction"]["value"],
            "timestamp": time.time(),
            "risk_flags": json.dumps(test_scenario["suspicious_transaction"]["flags"]),
            "scenario_test": "v3_integration_e2e"
        }
    
        errors = bq_client.insert_rows_json(bq_client.get_table(tx_table_id), [tx_data])
        assert len(errors) == 0
    
        # 2. Risk analysis updates wallet score (Patch 1: Bidirectional sync)
        updated_risk_score = 0.94
    
        # Update in Neo4j (simulating graph analysis)
        neo4j_driver = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
    
        try:
            with neo4j_driver.session() as session:
                session.run("""
                MERGE (w:Wallet {address: $address})
                SET w.risk_score = $risk_score,
                    w.last_analysis = timestamp(),
                    w.scenario_test = $scenario_test
                """, {
                    "address": test_scenario["wallet_address"],
                    "risk_score": updated_risk_score,
                    "scenario_test": "v3_integration_e2e"
                })
    
            # 3. Generate ZK-attested signal (Patch 2)
            signal_data = {
                "signal_id": f"e2e_signal_{int(time.time())}",
                "wallet_address": test_scenario["wallet_address"],
                "risk_score": updated_risk_score,
                "evidence": test_scenario["suspicious_transaction"]["flags"],
                "attested": True,
                "scenario_test": "v3_integration_e2e"
            }
    
            # 4. Generate explanation (Patch 3)
            explanation = f"Wallet {test_scenario['wallet_address']} risk increased to {updated_risk_score} due to large transaction to new counterparty."
    
            # 5. Execute automated actions (Patch 4)
            if updated_risk_score > 0.9:
                actions_executed = ["alert_generated", "position_monitoring", "compliance_review"]
            else:
                actions_executed = ["monitoring_enabled"]
    
            # 6. Voice alert (Patch 5) - simulated
            voice_alert_generated = updated_risk_score > 0.9
    
            # Store complete scenario result
            scenario_table_id = f"{dataset_id}.e2e_scenarios"
            scenario_schema = [
                bigquery.SchemaField("scenario_id", "STRING"),
                bigquery.SchemaField("wallet_address", "STRING"),
                bigquery.SchemaField("initial_risk", "FLOAT"),
                bigquery.SchemaField("final_risk", "FLOAT"),
                bigquery.SchemaField("signal_generated", "BOOLEAN"),
                bigquery.SchemaField("explanation", "STRING"),
                bigquery.SchemaField("actions_executed", "STRING"),
                bigquery.SchemaField("voice_alert", "BOOLEAN"),
                bigquery.SchemaField("completed_at", "FLOAT"),
                bigquery.SchemaField("scenario_test", "STRING"),
            ]
    
            scenario_table = bigquery.Table(f"{project_id}.{scenario_table_id}", schema=scenario_schema)
            bq_client.create_table(scenario_table, exists_ok=True)
    
            scenario_result = {
                "scenario_id": f"e2e_{int(time.time())}",
                "wallet_address": test_scenario["wallet_address"],
                "initial_risk": test_scenario["initial_risk_score"],
                "final_risk": updated_risk_score,
                "signal_generated": True,
                "explanation": explanation,
                "actions_executed": json.dumps(actions_executed),
                "voice_alert": voice_alert_generated,
                "completed_at": time.time(),
                "scenario_test": "v3_integration_e2e"
            }
    
            errors = bq_client.insert_rows_json(
                bq_client.get_table(scenario_table_id),
                [scenario_result]
            )
            assert len(errors) == 0
    
            # Verify complete workflow
            query = f"""
            SELECT * FROM `{project_id}.{scenario_table_id}`
            WHERE scenario_test = 'v3_integration_e2e'
            ORDER BY completed_at DESC
            LIMIT 1
            """
    
            query_job = bq_client.query(query)
            results = list(query_job.result())
    
            assert len(results) == 1
            result = results[0]
            assert result.final_risk > result.initial_risk
            assert result.signal_generated is True
            assert result.voice_alert is True
            assert len(json.loads(result.actions_executed)) >= 3
    
            print("‚úÖ V3 End-to-End Integration test passed")
    
        finally:
            # Cleanup Neo4j
            with neo4j_driver.session() as session:
>               session.run("""
                MATCH (w:Wallet {scenario_test: 'v3_integration_e2e'})
                DELETE w
                """)

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:751: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313: in run
    self._connect(self._config.default_access_mode)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/session.py:136: in _connect
    super()._connect(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:186: in _connect
    target_db = self._get_routing_target_database(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py:260: in _get_routing_target_database
    self._pool.update_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:971: in update_routing_table
    self._update_routing_table_from(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:902: in _update_routing_table_from
    new_routing_table = self.fetch_routing_table(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:815: in fetch_routing_table
    new_routing_info = self.fetch_routing_info(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:776: in fetch_routing_info
    cx = self._acquire(address, auth, deadline, None)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:408: in _acquire
    return connection_creator()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:230: in connection_creator
    connection = self.opener(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py:695: in opener
    return Bolt.open(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:430: in open
    connection.hello()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:740: in hello
    self.fetch_all()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:879: in fetch_all
    detail_delta, summary_delta = self.fetch_message()
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:864: in fetch_message
    res = self._process_message(tag, fields)
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208: in _process_message
    response.on_failure(summary_metadata or {})
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <neo4j._sync.io._common.LogonResponse object at 0x15a446190>
metadata = {'description': 'error: syntax error or access rule violation - permission/access denied. Access denied, see the secur...': 'CLIENT_ERROR'}, 'gql_status': '42NFF', 'message': 'The client is unauthorized due to authentication failure.', ...}

    def on_failure(self, metadata):
        # No sense in resetting the connection,
        # the server will have closed it already.
        self.connection.kill()
        handler = self.handlers.get("on_failure")
        Util.callback(handler, metadata)
        handler = self.handlers.get("on_summary")
        Util.callback(handler)
>       raise self._hydrate_error(metadata)
E       neo4j.exceptions.AuthError: {code: Neo.ClientError.Security.Unauthorized} {message: The client is unauthorized due to authentication failure.}

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:295: AuthError
=============================== warnings summary ===============================
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/legacy/__init__.py:6
  /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
    warnings.warn(  # deprecated in 14.0 - 2024-11-09

tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_rest_api_endpoints
  /Users/jadenfix/eth/services/dashboard/status_dashboard.py:529: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_rest_api_endpoints
tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_rest_api_endpoints
  /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/fastapi/applications.py:4495: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_rest_api_endpoints
  /Users/jadenfix/eth/services/dashboard/status_dashboard.py:535: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

tests/e2e/tier0/test_t0_a_basic_ingestion.py: 3 warnings
tests/e2e/tier0/test_t0_b_basic_queries.py: 3 warnings
tests/e2e/tier0/test_t0_d_ui_rendering.py: 1 warning
tests/e2e/tier1/test_t1_a_realtime_ingestion.py: 5 warnings
tests/e2e/tier1/test_t1_b_bidirectional_sync.py: 5 warnings
  /Users/jadenfix/eth/tests/e2e/helpers/gcp.py:69: PendingDeprecationWarning: Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
    table_ref = client.dataset(dataset_id).table(table_id)

tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_simple_bigquery_query
tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_aggregated_query
tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_query_with_filters
tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_dashboard_loads_without_crash
tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_sync_conflict_resolution
  /Users/jadenfix/eth/tests/e2e/helpers/gcp.py:91: PendingDeprecationWarning: Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
    table_ref = self.bq_client.dataset(dataset_id).table(table_id)

tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_bigquery_full_permissions
  /Users/jadenfix/.local/lib/python3.11/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_bigquery_full_permissions returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_vertex_ai_permissions
  /Users/jadenfix/.local/lib/python3.11/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_vertex_ai_permissions returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_comprehensive_gcp_access
  /Users/jadenfix/.local/lib/python3.11/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but tests/e2e/tier2/test_gcp_permissions_check.py::TestGCPPermissions::test_comprehensive_gcp_access returned (True, True), which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_4_autonomous_action_executor
tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_4_autonomous_action_executor
tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_4_autonomous_action_executor
  /Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/neo4j/_sync/driver.py:547: DeprecationWarning: Relying on Driver's destructor to close the session is deprecated. Please make sure to close the session. Use it as a context (`with` statement) or make sure to call `.close()` explicitly. Future versions of the driver will not close drivers automatically.
    _deprecation_warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
SKIPPED [1] tests/e2e/tier2/test_t2_a_real_service_integration.py:393: GraphQL endpoint returned 404
FAILED tests/e2e/test_comprehensive.py::TestLayer1Ingestion::test_ethereum_ingestion_pipeline
FAILED tests/e2e/test_comprehensive.py::TestLayer2SemanticFusion::test_ontology_graphql_api
FAILED tests/e2e/test_comprehensive.py::TestLayer3IntelligenceAgentMesh::test_vertex_ai_pipeline_mock
FAILED tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_graphql_api_endpoints
FAILED tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_voice_ops_integration
FAILED tests/e2e/test_comprehensive.py::TestLayer5UXWorkflowBuilder::test_dagster_workflow_execution
FAILED tests/e2e/test_comprehensive.py::TestLayer5UXWorkflowBuilder::test_custom_workflow_builder
FAILED tests/e2e/test_comprehensive.py::TestLayer6SystemIntegration::test_full_pipeline_integration
FAILED tests/e2e/test_comprehensive.py::TestLayer6SystemIntegration::test_health_monitoring_integration
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_encryption_at_rest
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_gdpr_compliance
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_soc2_audit_trail
FAILED tests/e2e/test_neo4j_connection.py::test_neo4j_connection - neo4j.exce...
FAILED tests/e2e/tier0/test_t0_a_basic_ingestion.py::TestIngestToBigQuery::test_ingest_synthetic_transaction
FAILED tests/e2e/tier0/test_t0_a_basic_ingestion.py::TestIngestToBigQuery::test_ingest_multiple_transactions
FAILED tests/e2e/tier0/test_t0_a_basic_ingestion.py::TestIngestToBigQuery::test_ingest_with_pubsub_simulation
FAILED tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_simple_bigquery_query
FAILED tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_aggregated_query
FAILED tests/e2e/tier0/test_t0_b_basic_queries.py::TestBigQueryQueries::test_query_with_filters
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_simple_graph_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_path_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_aggregation_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_export_format
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_dashboard_loads_without_crash
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_graph_visualization_endpoint
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_health_check_endpoint
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_api_error_handling
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_static_assets_loading
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_pubsub_to_bigquery_pipeline
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_high_volume_ingestion
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_data_validation_and_filtering
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_streaming_ingestion_latency
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_duplicate_detection
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_bigquery_to_neo4j_sync
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_neo4j_to_bigquery_sync
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_bidirectional_consistency_check
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_real_time_sync_latency
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_sync_conflict_resolution
FAILED tests/e2e/tier2/test_t2_a_real_service_integration.py::TestRealServiceIntegration::test_neo4j_graph_database_integration
FAILED tests/e2e/tier2/test_t2_a_real_service_integration.py::TestRealServiceIntegration::test_websocket_endpoints
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_1_bidirectional_graph_sync
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_2_zk_attested_signals
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_4_autonomous_action_executor
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_5_voice_ops_polish
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_v3_integration_end_to_end
============ 45 failed, 46 passed, 1 skipped, 33 warnings in 47.91s ============
