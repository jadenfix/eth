...F...F....FF..FFF.FF.FFF.............F...FFFFFFFFF...........FFFFFFFFF [ 54%]
F..........Fs....F..FF.F..FFFFFFFFFFFFF
=================================== FAILURES ===================================
_____________ TestLayer1Ingestion.test_ethereum_ingestion_pipeline _____________

self = <tests.e2e.test_comprehensive.TestLayer1Ingestion object at 0x11ad65410>
mock_blockchain_data = {'block_number': 18500000, 'timestamp': 1753476198, 'transactions': [{'from': '0xabc1230000000000000000000000000000000...'from': '0xmev_bot000000000000000000000000000000', 'gas': 500000, 'gasPrice': '200000000000', 'gasUsed': 450000, ...}]}

    @pytest.mark.asyncio
    async def test_ethereum_ingestion_pipeline(self, mock_blockchain_data):
        """Test complete Ethereum ingestion pipeline."""
        # Mock Web3 and Pub/Sub
        with patch('services.ethereum_ingester.ethereum_ingester.Web3') as mock_web3, \
             patch('google.cloud.pubsub_v1.PublisherClient') as mock_publisher:
    
            from services.ethereum_ingester.ethereum_ingester import EthereumIngester
    
            # Setup mocks
            mock_web3_instance = Mock()
            mock_web3_instance.eth.block_number = mock_blockchain_data['block_number']
            mock_web3_instance.eth.get_block.return_value = Mock(
                number=mock_blockchain_data['block_number'],
                timestamp=mock_blockchain_data['timestamp'],
                transactions=[Mock(**tx) for tx in mock_blockchain_data['transactions']]
            )
            mock_web3.return_value = mock_web3_instance
    
            mock_pub_client = Mock()
            mock_pub_client.publish.return_value = Mock()
            mock_publisher.return_value = mock_pub_client
    
            # Test ingester
            ingester = EthereumIngester()
            await ingester._process_block(mock_blockchain_data['block_number'])
    
            # Verify events were published
>           assert mock_pub_client.publish.called
E           AssertionError: assert False
E            +  where False = <Mock name='PublisherClient().publish' id='5012591568'>.called
E            +    where <Mock name='PublisherClient().publish' id='5012591568'> = <Mock name='PublisherClient()' id='4785467024'>.publish

tests/e2e/test_comprehensive.py:200: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:18.794560Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0x1234567890abcdef000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:18.794700Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:18.794778Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0xfedcba0987654321000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:18.794819Z"}
______________ TestLayer2SemanticFusion.test_ontology_graphql_api ______________

self = <tests.e2e.test_comprehensive.TestLayer2SemanticFusion object at 0x11ad67010>

    def test_ontology_graphql_api(self):
        """Test ontology GraphQL API."""
        from services.graph_api.graph_api import app
    
        client = TestClient(app)
    
        # Test basic schema query
        query = """
        query {
            __schema {
                types {
                    name
                }
            }
        }
        """
    
        response = client.post("/graphql", json={"query": query})
        assert response.status_code == 200
    
        data = response.json()
        type_names = [t['name'] for t in data['data']['__schema']['types']]
    
        # Verify core types exist
        assert 'Entity' in type_names
>       assert 'Address' in type_names
E       AssertionError: assert 'Address' in ['Query', 'String', 'Int', 'Mutation', 'Entity', 'Float', ...]

tests/e2e/test_comprehensive.py:302: AssertionError
_________ TestLayer3IntelligenceAgentMesh.test_vertex_ai_pipeline_mock _________

self = <tests.e2e.test_comprehensive.TestLayer3IntelligenceAgentMesh object at 0x11ad712d0>

    @pytest.mark.asyncio
    async def test_vertex_ai_pipeline_mock(self):
        """Test Vertex AI pipeline integration (mocked)."""
        from services.entity_resolution.pipeline import VertexAIPipeline
    
        with patch('google.cloud.aiplatform.PipelineJob') as mock_pipeline:
            pipeline = VertexAIPipeline()
    
            # Test pipeline execution
>           result = await pipeline.run_entity_resolution_job({
                'input_addresses': ['0xabc123' + '0' * 34],
                'confidence_threshold': 0.8
            })

tests/e2e/test_comprehensive.py:412: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
services/entity_resolution/pipeline.py:304: in run_entity_resolution_job
    PipelineJob('test-job', '/tmp/test-template.yaml')
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/pipeline_jobs.py:226: in __init__
    pipeline_json = yaml_utils.load_yaml(
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/utils/yaml_utils.py:65: in load_yaml
    return _load_yaml_from_local_file(path)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file_path = '/tmp/test-template.yaml'

    def _load_yaml_from_local_file(file_path: str) -> Dict[str, Any]:
        """Loads data from a YAML local file.
    
        Args:
          file_path (str):
              Required. The local file path of the YAML document.
    
        Returns:
          A Dict object representing the YAML document.
        """
        yaml = _maybe_import_yaml()
>       with open(file_path) as f:
E       FileNotFoundError: [Errno 2] No such file or directory: '/tmp/test-template.yaml'

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/google/cloud/aiplatform/utils/yaml_utils.py:116: FileNotFoundError
_______________ TestLayer4APIVoiceOps.test_graphql_api_endpoints _______________

self = <tests.e2e.test_comprehensive.TestLayer4APIVoiceOps object at 0x11ad71ad0>

    def test_graphql_api_endpoints(self):
        """Test GraphQL API functionality."""
        from services.graph_api.graph_api import app
    
        client = TestClient(app)
    
        # Test entity query
        query = """
        query GetEntities($limit: Int) {
            entities(limit: $limit) {
                id
                type
                addresses
                confidence
            }
        }
        """
    
        response = client.post("/graphql", json={
            "query": query,
            "variables": {"limit": 10}
        })
    
        assert response.status_code == 200
        data = response.json()
>       assert 'data' in data
E       assert 'data' in {'errors': [{'locations': [{'column': 22, 'line': 3}], 'message': "Unknown argument 'limit' on field 'Query.entities'....dress'?"}, {'locations': [{'column': 17, 'line': 7}], 'message': "Cannot query field 'confidence' on type 'Entity'."}]}

tests/e2e/test_comprehensive.py:450: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    ariadne:logger.py:21 Unknown argument 'limit' on field 'Query.entities'.

GraphQL request:3:22
2 |         query GetEntities($limit: Int) {
3 |             entities(limit: $limit) {
  |                      ^
4 |                 id
graphql.error.graphql_error.GraphQLError: Unknown argument 'limit' on field 'Query.entities'.

GraphQL request:3:22
2 |         query GetEntities($limit: Int) {
3 |             entities(limit: $limit) {
  |                      ^
4 |                 id
ERROR    ariadne:logger.py:21 Cannot query field 'addresses' on type 'Entity'. Did you mean 'address'?

GraphQL request:6:17
5 |                 type
6 |                 addresses
  |                 ^
7 |                 confidence
graphql.error.graphql_error.GraphQLError: Cannot query field 'addresses' on type 'Entity'. Did you mean 'address'?

GraphQL request:6:17
5 |                 type
6 |                 addresses
  |                 ^
7 |                 confidence
ERROR    ariadne:logger.py:21 Cannot query field 'confidence' on type 'Entity'.

GraphQL request:7:17
6 |                 addresses
7 |                 confidence
  |                 ^
8 |             }
graphql.error.graphql_error.GraphQLError: Cannot query field 'confidence' on type 'Entity'.

GraphQL request:7:17
6 |                 addresses
7 |                 confidence
  |                 ^
8 |             }
_______________ TestLayer4APIVoiceOps.test_voice_ops_integration _______________

self = <tests.e2e.test_comprehensive.TestLayer4APIVoiceOps object at 0x11ad72e10>

    @pytest.mark.asyncio
    async def test_voice_ops_integration(self):
        """Test voice operations (TTS/STT) with mocks."""
>       with patch('elevenlabs.generate') as mock_tts, \
             patch('speech_recognition.Recognizer') as mock_stt:

tests/e2e/test_comprehensive.py:504: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/unittest/mock.py:1437: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x14ce90850>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'elevenlabs' from '/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/elevenlabs/__init__.py'> does not have the attribute 'generate'

../.pyenv/versions/3.11.3/lib/python3.11/unittest/mock.py:1410: AttributeError
_________ TestLayer5UXWorkflowBuilder.test_dagster_workflow_execution __________

self = <tests.e2e.test_comprehensive.TestLayer5UXWorkflowBuilder object at 0x11ad73610>

    def test_dagster_workflow_execution(self):
        """Test Dagster workflow execution."""
>       from services.workflow_builder.sample_signal import high_value_transfer_monitor

tests/e2e/test_comprehensive.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
___________ TestLayer5UXWorkflowBuilder.test_custom_workflow_builder ___________

self = <tests.e2e.test_comprehensive.TestLayer5UXWorkflowBuilder object at 0x11ad73c90>

    def test_custom_workflow_builder(self):
        """Test dynamic workflow creation."""
>       from services.workflow_builder.sample_signal import build_custom_workflow

tests/e2e/test_comprehensive.py:545: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
__________ TestLayer6SystemIntegration.test_full_pipeline_integration __________

self = <tests.e2e.test_comprehensive.TestLayer6SystemIntegration object at 0x11ad7cbd0>
mock_blockchain_data = {'block_number': 18500000, 'timestamp': 1753476202, 'transactions': [{'from': '0xabc1230000000000000000000000000000000...'from': '0xmev_bot000000000000000000000000000000', 'gas': 500000, 'gasPrice': '200000000000', 'gasUsed': 450000, ...}]}
mock_entity_resolution_data = {'entities': [{'addresses': ['0xabc1230000000000000000000000000000000000', '0xdef4560000000000000000000000000000000000...0xmev_bot000000000000000000000000000000'], 'confidence': 0.87, 'entity_id': 'ENT_002', 'entity_type': 'MEV_BOT', ...}]}

    @pytest.mark.asyncio
    async def test_full_pipeline_integration(self, mock_blockchain_data, mock_entity_resolution_data):
        """Test complete end-to-end pipeline."""
        published_signals = []
    
        # Mock all external dependencies
        with patch('services.ethereum_ingester.ethereum_ingester.Web3') as mock_web3, \
             patch('google.cloud.pubsub_v1.PublisherClient') as mock_publisher, \
             patch('neo4j.GraphDatabase.driver') as mock_neo4j, \
             patch('services.entity_resolution.pipeline.joblib.load') as mock_ml:
    
            # Setup mocks
            mock_web3_instance = Mock()
            mock_web3_instance.eth.block_number = mock_blockchain_data['block_number']
            mock_web3_instance.eth.get_block.return_value = Mock(
                number=mock_blockchain_data['block_number'],
                timestamp=mock_blockchain_data['timestamp'],
                transactions=[Mock(**tx) for tx in mock_blockchain_data['transactions']]
            )
            mock_web3.return_value = mock_web3_instance
    
            mock_pub_client = Mock()
            mock_publisher.return_value = mock_pub_client
    
            # Capture published messages
            published_messages = []
            def capture_publish(topic, message):
                published_messages.append(json.loads(message.decode('utf-8')))
                return Mock()
            mock_pub_client.publish.side_effect = capture_publish
    
            # Run ingestion
            from services.ethereum_ingester.ethereum_ingester import EthereumIngester
            ingester = EthereumIngester()
            await ingester._process_block(mock_blockchain_data['block_number'])
    
            # Verify ingestion published events
>           assert len(published_messages) > 0
E           assert 0 > 0
E            +  where 0 = len([])

tests/e2e/test_comprehensive.py:625: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:22.386081Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0x1234567890abcdef000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:22.386208Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:206 {"service": "ethereum-ingester", "error": "Object of type Mock is not JSON serializable", "event": "Error publishing event", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:22.386276Z"}
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0xfedcba0987654321000000000000000000000000000000000000000000000000", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:43:22.386309Z"}
________ TestLayer6SystemIntegration.test_health_monitoring_integration ________

self = <tests.e2e.test_comprehensive.TestLayer6SystemIntegration object at 0x11ad7d250>

    def test_health_monitoring_integration(self):
        """Test system health monitoring."""
>       from services.monitoring.health_service import HealthMonitoringService

tests/e2e/test_comprehensive.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    System Health and Monitoring Service.
    
    Provides comprehensive health checks, performance metrics, and
    operational insights for the blockchain intelligence platform.
    """
    
    import os
    import asyncio
    import json
    import time
    import logging
    import psutil
    from typing import Dict, List, Optional, Any, Tuple
    from dataclasses import dataclass, asdict
    from datetime import datetime, timedelta
    from enum import Enum
    
    import structlog
    import aiohttp
>   import aioredis
E   ModuleNotFoundError: No module named 'aioredis'

services/monitoring/health_service.py:21: ModuleNotFoundError
________________ TestSecurityCompliance.test_encryption_at_rest ________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x11ad71750>

    def test_encryption_at_rest(self):
        """Test data encryption capabilities."""
>       from services.access_control.audit_sink import DataEncryption
E       ImportError: cannot import name 'DataEncryption' from 'services.access_control.audit_sink' (/Users/jadenfix/eth/services/access_control/audit_sink.py)

tests/e2e/test_comprehensive.py:687: ImportError
_________________ TestSecurityCompliance.test_gdpr_compliance __________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x11ad66c90>

    def test_gdpr_compliance(self):
        """Test GDPR data handling compliance."""
>       from services.access_control.audit_sink import GDPRCompliance
E       ImportError: cannot import name 'GDPRCompliance' from 'services.access_control.audit_sink' (/Users/jadenfix/eth/services/access_control/audit_sink.py)

tests/e2e/test_comprehensive.py:701: ImportError
_________________ TestSecurityCompliance.test_soc2_audit_trail _________________

self = <tests.e2e.test_comprehensive.TestSecurityCompliance object at 0x11ad7d1d0>

    def test_soc2_audit_trail(self):
        """Test SOC 2 Type II audit trail generation."""
        from services.access_control.audit_sink import AuditLogger
    
        logger = AuditLogger()
    
        # Generate audit entries
        entries = [
            logger.log_access('user1@company.com', 'sensitive_table', 'SELECT', 'SUCCESS'),
            logger.log_access('user2@company.com', 'sensitive_table', 'UPDATE', 'DENIED'),
            logger.log_access('admin@company.com', 'system_config', 'MODIFY', 'SUCCESS')
        ]
    
        # Test audit trail completeness
        for entry in entries:
            assert 'user' in entry
            assert 'resource' in entry
            assert 'action' in entry
            assert 'result' in entry
            assert 'timestamp' in entry
>           assert 'ip_address' in entry
E           AssertionError: assert 'ip_address' in {'action': 'SELECT', 'metadata': {}, 'resource': 'sensitive_table', 'result': 'SUCCESS', ...}

tests/e2e/test_comprehensive.py:735: AssertionError
___________ TestIngestToBigQuery.test_ingest_with_pubsub_simulation ____________

self = <tests.e2e.tier0.test_t0_a_basic_ingestion.TestIngestToBigQuery object at 0x11d1f8850>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476209')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x15b8b7a50>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x11ad67590>
clean_test_data = None

    def test_ingest_with_pubsub_simulation(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion pipeline with Pub/Sub simulation"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # Setup
        test_dataset = f"{gcp_env.test_prefix}_ingestion"
        test_table = "chain_events"
        test_topic = f"{gcp_env.test_prefix}_raw_events"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier0/test_t0_a_basic_ingestion.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15b8b3b10>
topic_id = 'test_1753476209_raw_events'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
________________ TestNeo4jGraphQueries.test_simple_graph_query _________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x11d212a90>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476220')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15bab1550>
clean_test_data = None

    def test_simple_graph_query(self, gcp_env, neo4j_utils, clean_test_data):
        """
        T0-C: Basic Neo4j query test
    
        Flow:
        1. Create test entities and relationships
        2. Query the graph structure
        3. Verify JSON response format
        4. Validate graph data integrity
        """
        # 1. Setup test graph data
        test_entities = [
            {
                "address": "0xT0C123",
                "type": "wallet",
                "risk_score": 0.2,
                "total_volume": 1000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "address": "0xT0C456",
                "type": "contract",
                "risk_score": 0.1,
                "total_volume": 5000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "address": "0xT0C789",
                "type": "wallet",
                "risk_score": 0.8,
                "total_volume": 500000,
                "fixture_id": "T0_C_graph"
            }
        ]
    
        test_relationships = [
            {
                "from_address": "0xT0C123",
                "to_address": "0xT0C456",
                "relationship_type": "INTERACTED_WITH",
                "transaction_count": 5,
                "total_value": 2000000,
                "fixture_id": "T0_C_graph"
            },
            {
                "from_address": "0xT0C456",
                "to_address": "0xT0C789",
                "relationship_type": "SENT_TO",
                "transaction_count": 2,
                "total_value": 1500000,
                "fixture_id": "T0_C_graph"
            }
        ]
    
        # Load test data into Neo4j
>       neo4j_utils.load_entities(test_entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:70: AttributeError
_________________ TestNeo4jGraphQueries.test_graph_path_query __________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x11d213190>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476220')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15b946190>
clean_test_data = None

    def test_graph_path_query(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph path queries"""
        # Setup a longer path for testing
        entities = [
            {"address": f"0xPATH{i:03d}", "type": "wallet", "risk_score": 0.1 * i, "fixture_id": "T0_C_path"}
            for i in range(4)
        ]
    
        relationships = [
            {
                "from_address": f"0xPATH{i:03d}",
                "to_address": f"0xPATH{i+1:03d}",
                "relationship_type": "SENT_TO",
                "transaction_count": 1,
                "total_value": 1000000,
                "fixture_id": "T0_C_path"
            }
            for i in range(3)
        ]
    
>       neo4j_utils.load_entities(entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:137: AttributeError
______________ TestNeo4jGraphQueries.test_graph_aggregation_query ______________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x11d213890>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476220')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15ba29b50>
clean_test_data = None

    def test_graph_aggregation_query(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph aggregation queries"""
        # Create a hub node with multiple connections
        hub_entity = {
            "address": "0xHUB001",
            "type": "contract",
            "risk_score": 0.5,
            "fixture_id": "T0_C_agg"
        }
    
        spoke_entities = [
            {
                "address": f"0xSPOKE{i:02d}",
                "type": "wallet",
                "risk_score": 0.1 * i,
                "fixture_id": "T0_C_agg"
            }
            for i in range(5)
        ]
    
        relationships = [
            {
                "from_address": f"0xSPOKE{i:02d}",
                "to_address": "0xHUB001",
                "relationship_type": "SENT_TO",
                "transaction_count": i + 1,
                "total_value": (i + 1) * 1000000,
                "fixture_id": "T0_C_agg"
            }
            for i in range(5)
        ]
    
>       neo4j_utils.load_entities([hub_entity] + spoke_entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:190: AttributeError
________________ TestNeo4jGraphQueries.test_graph_export_format ________________

self = <tests.e2e.tier0.test_t0_c_graph_queries.TestNeo4jGraphQueries object at 0x11d220050>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476220')
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15b96c4d0>
clean_test_data = None

    def test_graph_export_format(self, gcp_env, neo4j_utils, clean_test_data):
        """Test graph data export in proper JSON format"""
        # Create simple test graph
        entities = [
            {"address": "0xEXPORT1", "type": "wallet", "label": "User Wallet", "fixture_id": "T0_C_export"},
            {"address": "0xEXPORT2", "type": "contract", "label": "DeFi Protocol", "fixture_id": "T0_C_export"}
        ]
    
        relationships = [
            {
                "from_address": "0xEXPORT1",
                "to_address": "0xEXPORT2",
                "relationship_type": "INTERACTED_WITH",
                "weight": 0.8,
                "fixture_id": "T0_C_export"
            }
        ]
    
>       neo4j_utils.load_entities(entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier0/test_t0_c_graph_queries.py:234: AttributeError
______________ TestUIRendering.test_dashboard_loads_without_crash ______________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x11d222910>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476220')
async_http_client = <async_generator object async_http_client at 0x15b8e1d20>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_dashboard_loads_without_crash(self, gcp_env, async_http_client, clean_test_data):
        """
        T0-D: Basic UI loading test
    
        Flow:
        1. Setup test data in backend
        2. Make request to dashboard endpoint
        3. Verify response is valid HTML/JSON
        4. Check for critical UI elements
        """
        # 1. Setup minimal test data for UI
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_ui_test"
        test_table = "dashboard_data"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, {
            "fields": [
                {"name": "metric_name", "type": "STRING"},
                {"name": "metric_value", "type": "FLOAT"},
                {"name": "timestamp", "type": "INTEGER"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Insert sample dashboard metrics
        dashboard_metrics = [
            {"metric_name": "total_transactions", "metric_value": 12345.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "total_volume", "metric_value": 9876543.21, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "risk_alerts", "metric_value": 23.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"},
            {"metric_name": "active_addresses", "metric_value": 4567.0, "timestamp": 1698000000, "fixture_id": "T0_D_ui"}
        ]
    
        gcp_utils.bq_insert_rows(test_dataset, test_table, dashboard_metrics)
    
        # 2. Test dashboard API endpoint
>       response = await async_http_client.get("/api/dashboard/metrics")
E       AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:54: AttributeError
______________ TestUIRendering.test_graph_visualization_endpoint _______________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x11d222fd0>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476221')
async_http_client = <async_generator object async_http_client at 0x15b968040>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_graph_visualization_endpoint(self, gcp_env, async_http_client, clean_test_data):
        """Test graph visualization endpoint"""
        # Test graph visualization API
>       response = await async_http_client.get("/api/graph/visualization", params={
            "address": "0xTEST123",
            "depth": 2
        })
E       AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:83: AttributeError
__________________ TestUIRendering.test_health_check_endpoint __________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x11d223690>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476221')
async_http_client = <async_generator object async_http_client at 0x15b969380>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_health_check_endpoint(self, gcp_env, async_http_client, clean_test_data):
        """Test application health check"""
>       response = await async_http_client.get("/health")
E       AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:100: AttributeError
___________________ TestUIRendering.test_api_error_handling ____________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x11d223d90>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476221')
async_http_client = <async_generator object async_http_client at 0x15b8e1700>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_api_error_handling(self, gcp_env, async_http_client, clean_test_data):
        """Test API error handling doesn't crash"""
        # Test invalid endpoints
        invalid_endpoints = [
            "/api/nonexistent",
            "/api/graph/invalid",
            "/api/dashboard/badparam"
        ]
    
        for endpoint in invalid_endpoints:
>           response = await async_http_client.get(endpoint)
E           AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:135: AttributeError
__________________ TestUIRendering.test_static_assets_loading __________________

self = <tests.e2e.tier0.test_t0_d_ui_rendering.TestUIRendering object at 0x11d213d50>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476221')
async_http_client = <async_generator object async_http_client at 0x15b8e2ea0>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_static_assets_loading(self, gcp_env, async_http_client, clean_test_data):
        """Test that static assets load properly"""
        # Test common static asset paths
        static_paths = [
            "/static/css/main.css",
            "/static/js/app.js",
            "/assets/logo.png",
            "/favicon.ico"
        ]
    
        loaded_assets = 0
    
        for path in static_paths:
>           response = await async_http_client.get(path)
E           AttributeError: 'async_generator' object has no attribute 'get'

tests/e2e/tier0/test_t0_d_ui_rendering.py:166: AttributeError
____________ TestRealTimeIngestion.test_pubsub_to_bigquery_pipeline ____________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x11d2ae610>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476221')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x15b959090>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15ba2a510>
sample_chain_event = {'block_number': 18500000, 'event_type': 'transfer', 'fixture_id': 'test_sample_event', 'from_address': '0xA0b86a33E6441e8C73C3238E5A3F0B2E1f1D8E3F', ...}
clean_test_data = None

    def test_pubsub_to_bigquery_pipeline(self, gcp_env, pubsub_publisher, bigquery_client, sample_chain_event, clean_test_data):
        """
        T1-A: End-to-end ingestion pipeline test
    
        Flow:
        1. Setup Pub/Sub topic and BigQuery destination
        2. Publish Ethereum event to Pub/Sub
        3. Simulate Dataflow processing
        4. Verify data appears correctly in BigQuery
        5. Validate data transformation and enrichment
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup pipeline infrastructure
        test_dataset = f"{gcp_env.test_prefix}_realtime_ingestion"
        test_table = "ethereum_events"
        test_topic = f"{gcp_env.test_prefix}_ethereum_raw"
        test_subscription = f"{gcp_env.test_prefix}_ethereum_processor"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15ba2aa90>
topic_id = 'test_1753476221_ethereum_raw'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
_______________ TestRealTimeIngestion.test_high_volume_ingestion _______________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x11d2af4d0>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476223')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x11d212c10>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15b9457d0>
clean_test_data = None

    def test_high_volume_ingestion(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion pipeline under load"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_volume_test"
        test_table = "high_volume_events"
        test_topic = f"{gcp_env.test_prefix}_volume_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15b945590>
topic_id = 'test_1753476223_volume_raw'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
___________ TestRealTimeIngestion.test_data_validation_and_filtering ___________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x11d2ac390>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476224')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x15b8aa250>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15b87da90>
clean_test_data = None

    def test_data_validation_and_filtering(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test data validation and filtering in ingestion pipeline"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_validation"
        test_table = "validated_events"
        test_topic = f"{gcp_env.test_prefix}_validation_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15b87e050>
topic_id = 'test_1753476224_validation_raw'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
____________ TestRealTimeIngestion.test_streaming_ingestion_latency ____________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x11d2ace10>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476225')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x15b90b710>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15b901250>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_streaming_ingestion_latency(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test ingestion latency for streaming data"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_latency"
        test_table = "latency_events"
        test_topic = f"{gcp_env.test_prefix}_latency_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15b900b90>
topic_id = 'test_1753476225_latency_raw'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
________________ TestRealTimeIngestion.test_duplicate_detection ________________

self = <tests.e2e.tier1.test_t1_a_realtime_ingestion.TestRealTimeIngestion object at 0x11d2ac990>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476226')
pubsub_publisher = <google.cloud.pubsub_v1.PublisherClient object at 0x15b895b50>
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15ba22910>
clean_test_data = None

    def test_duplicate_detection(self, gcp_env, pubsub_publisher, bigquery_client, clean_test_data):
        """Test duplicate event detection and handling"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_dedup"
        test_table = "dedup_events"
        test_topic = f"{gcp_env.test_prefix}_dedup_raw"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, test_table, CHAIN_EVENTS_SCHEMA)
>       gcp_utils.pubsub_create_topic(test_topic)

tests/e2e/tier1/test_t1_a_realtime_ingestion.py:350: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15ba22950>
topic_id = 'test_1753476226_dedup_raw'

    def pubsub_create_topic(self, topic_id: str) -> None:
        """Create Pub/Sub topic if it doesn't exist"""
>       topic_path = self.publisher.topic_path(self.project_id, topic_id)
E       AttributeError: 'GCPTestUtils' object has no attribute 'publisher'

tests/e2e/helpers/gcp.py:139: AttributeError
______________ TestBidirectionalSync.test_bigquery_to_neo4j_sync _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x11d291350>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476227')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15ba22410>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15ba7ef10>
clean_test_data = None

    def test_bigquery_to_neo4j_sync(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """
        T1-B: BigQuery → Neo4j synchronization
    
        Flow:
        1. Insert entity data into BigQuery
        2. Trigger sync process (CDC simulation)
        3. Verify entities appear in Neo4j
        4. Validate relationship creation
        5. Check data consistency
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup BigQuery entities table
        test_dataset = f"{gcp_env.test_prefix}_sync_test"
        entities_table = "entities"
        relationships_table = "relationships"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)
        gcp_utils.bq_create_table(test_dataset, relationships_table, {
            "fields": [
                {"name": "from_address", "type": "STRING"},
                {"name": "to_address", "type": "STRING"},
                {"name": "relationship_type", "type": "STRING"},
                {"name": "transaction_count", "type": "INTEGER"},
                {"name": "total_value", "type": "FLOAT"},
                {"name": "first_seen", "type": "INTEGER"},
                {"name": "last_seen", "type": "INTEGER"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Insert test entities to BigQuery
        test_entities = [
            {
                "address": "0xBQ2NEO001",
                "entity_type": "wallet",
                "risk_score": 0.3,
                "total_volume": 5000000.0,
                "transaction_count": 25,
                "first_seen": 1690000000,
                "last_seen": 1698000000,
                "labels": json.dumps(["high_volume", "defi_user"]),
                "fixture_id": "T1_B_bq_to_neo"
            },
            {
                "address": "0xBQ2NEO002",
                "entity_type": "contract",
                "risk_score": 0.1,
                "total_volume": 50000000.0,
                "transaction_count": 1000,
                "first_seen": 1680000000,
                "last_seen": 1698000000,
                "labels": json.dumps(["dex", "verified"]),
                "fixture_id": "T1_B_bq_to_neo"
            },
            {
                "address": "0xBQ2NEO003",
                "entity_type": "wallet",
                "risk_score": 0.9,
                "total_volume": 100000.0,
                "transaction_count": 5,
                "first_seen": 1697000000,
                "last_seen": 1698000000,
                "labels": json.dumps(["suspicious", "new_account"]),
                "fixture_id": "T1_B_bq_to_neo"
            }
        ]
    
        test_relationships = [
            {
                "from_address": "0xBQ2NEO001",
                "to_address": "0xBQ2NEO002",
                "relationship_type": "INTERACTED_WITH",
                "transaction_count": 10,
                "total_value": 1500000.0,
                "first_seen": 1695000000,
                "last_seen": 1698000000,
                "fixture_id": "T1_B_bq_to_neo"
            },
            {
                "from_address": "0xBQ2NEO003",
                "to_address": "0xBQ2NEO002",
                "relationship_type": "SENT_TO",
                "transaction_count": 3,
                "total_value": 75000.0,
                "first_seen": 1697500000,
                "last_seen": 1698000000,
                "fixture_id": "T1_B_bq_to_neo"
            }
        ]
    
>       gcp_utils.bq_insert_rows(test_dataset, entities_table, test_entities)

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15b92dad0>
dataset_id = 'test_1753476227_sync_test', table_id = 'entities'
rows = [{'address': '0xBQ2NEO001', 'entity_type': 'wallet', 'first_seen': 1690000000, 'fixture_id': 'T1_B_bq_to_neo', ...}, {....}, {'address': '0xBQ2NEO003', 'entity_type': 'wallet', 'first_seen': 1697000000, 'fixture_id': 'T1_B_bq_to_neo', ...}]

    def bq_insert_rows(self, dataset_id: str, table_id: str, rows: List[Dict]) -> None:
        """Insert rows into BigQuery table"""
        table_ref = self.bq_client.dataset(dataset_id).table(table_id)
        table = self.bq_client.get_table(table_ref)
    
        errors = self.bq_client.insert_rows_json(table, rows)
        if errors:
>           raise Exception(f"BigQuery insert errors: {errors}")
E           Exception: BigQuery insert errors: [{'index': 0, 'errors': [{'reason': 'invalid', 'location': 'last_seen', 'debugInfo': '', 'message': 'no such field: last_seen.'}]}, {'index': 1, 'errors': [{'reason': 'invalid', 'location': 'last_seen', 'debugInfo': '', 'message': 'no such field: last_seen.'}]}, {'index': 2, 'errors': [{'reason': 'invalid', 'location': 'fixture_id', 'debugInfo': '', 'message': 'no such field: fixture_id.'}]}]

tests/e2e/helpers/gcp.py:108: Exception
______________ TestBidirectionalSync.test_neo4j_to_bigquery_sync _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x11d291b10>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476229')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15ba65890>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15b92d390>
clean_test_data = None

    def test_neo4j_to_bigquery_sync(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """
        T1-B: Neo4j → BigQuery synchronization
    
        Flow:
        1. Create entities and relationships in Neo4j
        2. Trigger reverse sync process
        3. Verify data appears in BigQuery
        4. Check data transformation and enrichment
        """
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # 1. Setup BigQuery destination tables
        test_dataset = f"{gcp_env.test_prefix}_reverse_sync"
        entities_table = "neo4j_entities"
        relationships_table = "neo4j_relationships"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)
        gcp_utils.bq_create_table(test_dataset, relationships_table, {
            "fields": [
                {"name": "from_address", "type": "STRING"},
                {"name": "to_address", "type": "STRING"},
                {"name": "relationship_type", "type": "STRING"},
                {"name": "weight", "type": "FLOAT"},
                {"name": "properties", "type": "STRING"},
                {"name": "created_in_neo4j", "type": "INTEGER"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # 2. Create test data in Neo4j
        neo4j_entities = [
            {
                "address": "0xNEO2BQ001",
                "type": "wallet",
                "risk_score": 0.4,
                "total_volume": 2000000,
                "clustering_coefficient": 0.6,
                "centrality_score": 0.8,
                "fixture_id": "T1_B_neo_to_bq"
            },
            {
                "address": "0xNEO2BQ002",
                "type": "contract",
                "risk_score": 0.2,
                "total_volume": 20000000,
                "clustering_coefficient": 0.9,
                "centrality_score": 0.95,
                "fixture_id": "T1_B_neo_to_bq"
            }
        ]
    
        neo4j_relationships = [
            {
                "from_address": "0xNEO2BQ001",
                "to_address": "0xNEO2BQ002",
                "relationship_type": "COMPLEX_INTERACTION",
                "weight": 0.85,
                "interaction_patterns": ["frequent", "large_amounts"],
                "risk_indicators": ["none"],
                "fixture_id": "T1_B_neo_to_bq"
            }
        ]
    
>       neo4j_utils.load_entities(neo4j_entities)
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:257: AttributeError
__________ TestBidirectionalSync.test_bidirectional_consistency_check __________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x11d2ad410>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476230')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15b87da90>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15ba541d0>
clean_test_data = None

    def test_bidirectional_consistency_check(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test data consistency between BigQuery and Neo4j after bidirectional sync"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        # Setup test environment
        test_dataset = f"{gcp_env.test_prefix}_consistency"
        entities_table = "entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)
    
        # Create test entity that will be synced both ways
        test_entity = {
            "address": "0xCONSISTENCY001",
            "entity_type": "wallet",
            "risk_score": 0.5,
            "total_volume": 1000000.0,
            "transaction_count": 10,
            "first_seen": 1690000000,
            "last_seen": 1698000000,
            "labels": json.dumps(["test", "consistency"]),
            "fixture_id": "T1_B_consistency"
        }
    
        # 1. Insert to BigQuery first
>       gcp_utils.bq_insert_rows(test_dataset, entities_table, [test_entity])

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:370: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15ba54690>
dataset_id = 'test_1753476230_consistency', table_id = 'entities'
rows = [{'address': '0xCONSISTENCY001', 'entity_type': 'wallet', 'first_seen': 1690000000, 'fixture_id': 'T1_B_consistency', ...}]

    def bq_insert_rows(self, dataset_id: str, table_id: str, rows: List[Dict]) -> None:
        """Insert rows into BigQuery table"""
        table_ref = self.bq_client.dataset(dataset_id).table(table_id)
        table = self.bq_client.get_table(table_ref)
    
        errors = self.bq_client.insert_rows_json(table, rows)
        if errors:
>           raise Exception(f"BigQuery insert errors: {errors}")
E           Exception: BigQuery insert errors: [{'index': 0, 'errors': [{'reason': 'invalid', 'location': 'last_seen', 'debugInfo': '', 'message': 'no such field: last_seen.'}]}]

tests/e2e/helpers/gcp.py:108: Exception
______________ TestBidirectionalSync.test_real_time_sync_latency _______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x11d2bda50>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476232')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15bb70d90>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15bb700d0>
clean_test_data = None

    @pytest.mark.asyncio
    async def test_real_time_sync_latency(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test latency of real-time bidirectional sync"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_sync_latency"
        entities_table = "real_time_entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, ENTITIES_SCHEMA)
    
        # Track sync timing
        sync_times = []
    
        for i in range(5):
            # 1. Insert to BigQuery with timestamp
            insert_time = time.time()
            entity = {
                "address": f"0xLATENCY{i:03d}",
                "entity_type": "wallet",
                "risk_score": 0.1 * i,
                "total_volume": 1000000.0 * i,
                "transaction_count": 10 * i,
                "first_seen": int(insert_time),
                "last_seen": int(insert_time),
                "labels": json.dumps([f"test_{i}"]),
                "fixture_id": "T1_B_latency"
            }
    
>           gcp_utils.bq_insert_rows(test_dataset, entities_table, [entity])

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.e2e.helpers.gcp.GCPTestUtils object at 0x15bb70c50>
dataset_id = 'test_1753476232_sync_latency', table_id = 'real_time_entities'
rows = [{'address': '0xLATENCY000', 'entity_type': 'wallet', 'first_seen': 1753476233, 'fixture_id': 'T1_B_latency', ...}]

    def bq_insert_rows(self, dataset_id: str, table_id: str, rows: List[Dict]) -> None:
        """Insert rows into BigQuery table"""
        table_ref = self.bq_client.dataset(dataset_id).table(table_id)
        table = self.bq_client.get_table(table_ref)
    
        errors = self.bq_client.insert_rows_json(table, rows)
        if errors:
>           raise Exception(f"BigQuery insert errors: {errors}")
E           Exception: BigQuery insert errors: [{'index': 0, 'errors': [{'reason': 'invalid', 'location': 'first_seen', 'debugInfo': '', 'message': 'no such field: first_seen.'}]}]

tests/e2e/helpers/gcp.py:108: Exception
_____________ TestBidirectionalSync.test_sync_conflict_resolution ______________

self = <tests.e2e.tier1.test_t1_b_bidirectional_sync.TestBidirectionalSync object at 0x11d290ad0>
gcp_env = GCPTestEnvironment(project_id='ethhackathon', dataset_id='test_onchain_data', region='us-central1', test_prefix='test_1753476234')
bigquery_client = <google.cloud.bigquery.client.Client object at 0x15ba19e10>
neo4j_utils = <tests.e2e.helpers.neo4j.Neo4jTestUtils object at 0x15ba18a90>
clean_test_data = None

    def test_sync_conflict_resolution(self, gcp_env, bigquery_client, neo4j_utils, clean_test_data):
        """Test conflict resolution when same entity is modified in both stores"""
        gcp_utils = GCPTestUtils(gcp_env.project_id)
    
        test_dataset = f"{gcp_env.test_prefix}_conflicts"
        entities_table = "conflict_entities"
    
        gcp_utils.bq_create_dataset(test_dataset)
        gcp_utils.bq_create_table(test_dataset, entities_table, {
            "fields": [
                {"name": "address", "type": "STRING"},
                {"name": "entity_type", "type": "STRING"},
                {"name": "risk_score", "type": "FLOAT"},
                {"name": "total_volume", "type": "FLOAT"},
                {"name": "last_modified", "type": "INTEGER"},
                {"name": "modified_in", "type": "STRING"},
                {"name": "fixture_id", "type": "STRING"}
            ]
        })
    
        # Create initial entity
        base_entity = {
            "address": "0xCONFLICT001",
            "entity_type": "wallet",
            "risk_score": 0.5,
            "total_volume": 1000000.0,
            "last_modified": int(time.time()),
            "modified_in": "initial",
            "fixture_id": "T1_B_conflict"
        }
    
        # Insert to both stores
        gcp_utils.bq_insert_rows(test_dataset, entities_table, [base_entity])
    
        neo4j_entity = {
            "address": base_entity["address"],
            "type": base_entity["entity_type"],
            "risk_score": base_entity["risk_score"],
            "total_volume": base_entity["total_volume"],
            "fixture_id": base_entity["fixture_id"]
        }
>       neo4j_utils.load_entities([neo4j_entity])
E       AttributeError: 'Neo4jTestUtils' object has no attribute 'load_entities'

tests/e2e/tier1/test_t1_b_bidirectional_sync.py:579: AttributeError
_____________ TestRealServiceIntegration.test_websocket_endpoints ______________

self = <tests.e2e.tier2.test_t2_a_real_service_integration.TestRealServiceIntegration object at 0x11d282fd0>

    @pytest.mark.asyncio
    async def test_websocket_endpoints(self):
        """Test WebSocket endpoints for real-time data"""
        ws_endpoint = os.getenv('NEXT_PUBLIC_WEBSOCKET_ENDPOINT', 'ws://localhost:4000/subscriptions')
    
        try:
>           async with websockets.connect(ws_endpoint, timeout=10) as websocket:

tests/e2e/tier2/test_t2_a_real_service_integration.py:345: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:587: in __aenter__
    return await self
../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:541: in __await_impl__
    self.connection = await self.create_connection()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <websockets.asyncio.client.connect object at 0x15bb15fd0>

    async def create_connection(self) -> ClientConnection:
        """Create TCP or Unix connection."""
        loop = asyncio.get_running_loop()
        kwargs = self.connection_kwargs.copy()
    
        ws_uri = parse_uri(self.uri)
    
        proxy = self.proxy
        if kwargs.get("unix", False):
            proxy = None
        if kwargs.get("sock") is not None:
            proxy = None
        if proxy is True:
            proxy = get_proxy(ws_uri)
    
        def factory() -> ClientConnection:
            return self.protocol_factory(ws_uri)
    
        if ws_uri.secure:
            kwargs.setdefault("ssl", True)
            kwargs.setdefault("server_hostname", ws_uri.host)
            if kwargs.get("ssl") is None:
                raise ValueError("ssl=None is incompatible with a wss:// URI")
        else:
            if kwargs.get("ssl") is not None:
                raise ValueError("ssl argument is incompatible with a ws:// URI")
    
        if kwargs.pop("unix", False):
            _, connection = await loop.create_unix_connection(factory, **kwargs)
        elif proxy is not None:
            proxy_parsed = parse_proxy(proxy)
            if proxy_parsed.scheme[:5] == "socks":
                # Connect to the server through the proxy.
                sock = await connect_socks_proxy(
                    proxy_parsed,
                    ws_uri,
                    local_addr=kwargs.pop("local_addr", None),
                )
                # Initialize WebSocket connection via the proxy.
                _, connection = await loop.create_connection(
                    factory,
                    sock=sock,
                    **kwargs,
                )
            elif proxy_parsed.scheme[:4] == "http":
                # Split keyword arguments between the proxy and the server.
                all_kwargs, proxy_kwargs, kwargs = kwargs, {}, {}
                for key, value in all_kwargs.items():
                    if key.startswith("ssl") or key == "server_hostname":
                        kwargs[key] = value
                    elif key.startswith("proxy_"):
                        proxy_kwargs[key[6:]] = value
                    else:
                        proxy_kwargs[key] = value
                # Validate the proxy_ssl argument.
                if proxy_parsed.scheme == "https":
                    proxy_kwargs.setdefault("ssl", True)
                    if proxy_kwargs.get("ssl") is None:
                        raise ValueError(
                            "proxy_ssl=None is incompatible with an https:// proxy"
                        )
                else:
                    if proxy_kwargs.get("ssl") is not None:
                        raise ValueError(
                            "proxy_ssl argument is incompatible with an http:// proxy"
                        )
                # Connect to the server through the proxy.
                transport = await connect_http_proxy(
                    proxy_parsed,
                    ws_uri,
                    user_agent_header=self.user_agent_header,
                    **proxy_kwargs,
                )
                # Initialize WebSocket connection via the proxy.
                connection = factory()
                transport.set_protocol(connection)
                ssl = kwargs.pop("ssl", None)
                if ssl is True:
                    ssl = ssl_module.create_default_context()
                if ssl is not None:
                    new_transport = await loop.start_tls(
                        transport, connection, ssl, **kwargs
                    )
                    assert new_transport is not None  # help mypy
                    transport = new_transport
                connection.connection_made(transport)
            else:
                raise AssertionError("unsupported proxy")
        else:
            # Connect to the server directly.
            if kwargs.get("sock") is None:
                kwargs.setdefault("host", ws_uri.host)
                kwargs.setdefault("port", ws_uri.port)
            # Initialize WebSocket connection.
>           _, connection = await loop.create_connection(factory, **kwargs)
E           TypeError: BaseEventLoop.create_connection() got an unexpected keyword argument 'timeout'

../.pyenv/versions/3.11.3/lib/python3.11/site-packages/websockets/asyncio/client.py:467: TypeError
_______ TestV3PatchesIntegration.test_patch_4_autonomous_action_executor _______

self = <tests.e2e.tier2.test_t2_b_v3_patches_integration.TestV3PatchesIntegration object at 0x11d1f3790>

    def test_patch_4_autonomous_action_executor(self):
        """
        Patch 4: Autonomous Action Executor
        Test automated response system with YAML playbooks
        """
        # Create test playbook configuration
        test_playbook = {
            "name": "high_risk_wallet_response",
            "trigger": {
                "risk_score_threshold": 0.9,
                "confidence_threshold": 0.8
            },
            "actions": [
                {
                    "type": "freeze_position",
                    "params": {
                        "duration": "24h",
                        "notify_compliance": True
                    }
                },
                {
                    "type": "hedge_exposure",
                    "params": {
                        "percentage": 50,
                        "instruments": ["USDC", "ETH"]
                    }
                },
                {
                    "type": "alert_notification",
                    "params": {
                        "channels": ["slack", "email"],
                        "priority": "high"
                    }
                }
            ]
        }
    
        # Simulate high-risk signal that triggers action executor
        trigger_signal = {
            "signal_id": f"action_test_{int(time.time())}",
            "wallet_address": "0xhighrisk789",
            "risk_score": 0.95,
            "confidence": 0.92,
            "signal_type": "money_laundering_detected",
            "evidence": ["mixer_interaction", "rapid_transactions", "suspicious_amounts"],
            "timestamp": time.time()
        }
    
        # Test action execution simulation
        executed_actions = []
    
        for action in test_playbook["actions"]:
            if action["type"] == "freeze_position":
                # Simulate position freeze
                freeze_result = {
                    "action_type": "freeze_position",
                    "wallet_address": trigger_signal["wallet_address"],
                    "duration": action["params"]["duration"],
                    "status": "executed",
                    "timestamp": time.time(),
                    "dry_run": True  # Safety flag
                }
                executed_actions.append(freeze_result)
    
            elif action["type"] == "hedge_exposure":
                # Simulate hedge execution
                hedge_result = {
                    "action_type": "hedge_exposure",
                    "wallet_address": trigger_signal["wallet_address"],
                    "hedge_percentage": action["params"]["percentage"],
                    "instruments": action["params"]["instruments"],
                    "status": "executed",
                    "timestamp": time.time(),
                    "dry_run": True
                }
                executed_actions.append(hedge_result)
    
            elif action["type"] == "alert_notification":
                # Simulate notification sending
                alert_result = {
                    "action_type": "alert_notification",
                    "wallet_address": trigger_signal["wallet_address"],
                    "channels": action["params"]["channels"],
                    "priority": action["params"]["priority"],
                    "status": "sent",
                    "timestamp": time.time()
                }
                executed_actions.append(alert_result)
    
        # Store action execution results
        project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        dataset_id = os.getenv('BIGQUERY_DATASET')
        bq_client = bigquery.Client(project=project_id)
    
        table_id = f"{dataset_id}.action_executions"
        schema = [
            bigquery.SchemaField("signal_id", "STRING"),
            bigquery.SchemaField("wallet_address", "STRING"),
            bigquery.SchemaField("action_type", "STRING"),
            bigquery.SchemaField("status", "STRING"),
            bigquery.SchemaField("execution_details", "STRING"),
            bigquery.SchemaField("executed_at", "FLOAT"),
            bigquery.SchemaField("dry_run", "BOOLEAN"),
            bigquery.SchemaField("patch_test", "STRING"),
        ]
    
        table = bigquery.Table(f"{project_id}.{table_id}", schema=schema)
        bq_client.create_table(table, exists_ok=True)
    
        # Insert execution records
        execution_records = []
        for action in executed_actions:
            record = {
                "signal_id": trigger_signal["signal_id"],
                "wallet_address": trigger_signal["wallet_address"],
                "action_type": action["action_type"],
                "status": action["status"],
                "execution_details": json.dumps(action),
                "executed_at": action["timestamp"],
                "dry_run": action.get("dry_run", False),
                "patch_test": "patch_4_action_executor"
            }
            execution_records.append(record)
    
        errors = bq_client.insert_rows_json(
            bq_client.get_table(table_id),
            execution_records
        )
        assert len(errors) == 0
    
        # Verify actions were executed
        query = f"""
        SELECT COUNT(*) as action_count,
               COUNTIF(status = 'executed' OR status = 'sent') as successful_actions
        FROM `{project_id}.{table_id}`
        WHERE patch_test = 'patch_4_action_executor'
        """
    
        query_job = bq_client.query(query)
        results = list(query_job.result())
    
        assert len(results) == 1
>       assert results[0].action_count == 3  # All three actions
E       AssertionError: assert 51 == 3
E        +  where 51 = Row((51, 51), {'action_count': 0, 'successful_actions': 1}).action_count

tests/e2e/tier2/test_t2_b_v3_patches_integration.py:485: AssertionError
______________ TestIngestionToProcessing.test_pubsub_message_flow ______________

self = <test_services_integration.TestIngestionToProcessing object at 0x11d466010>
sample_blockchain_events = [{'block_number': 18500000, 'event_name': 'TRANSFER', 'from_address': '0xsender12300000000000000000000000000000', 'gas...00001, 'event_name': 'SWAP', 'from_address': '0xmevbot45600000000000000000000000000000', 'gas_price_gwei': 200.0, ...}]

    @pytest.mark.asyncio
    async def test_pubsub_message_flow(self, sample_blockchain_events):
        """Test Pub/Sub message publishing and consuming."""
        published_messages = []
    
        # Mock Pub/Sub publisher
        with patch('google.cloud.pubsub_v1.PublisherClient') as mock_pub:
            mock_client = Mock()
    
            def capture_publish(topic, data, **kwargs):
                published_messages.append(json.loads(data.decode('utf-8')))
                return Mock()
    
            mock_client.publish.side_effect = capture_publish
            mock_pub.return_value = mock_client
    
            # Simulate ingestion service
>           from services.ethereum_ingester.ethereum_ingester import MessagePublisher
E           ImportError: cannot import name 'MessagePublisher' from 'services.ethereum_ingester.ethereum_ingester' (/Users/jadenfix/eth/services/ethereum_ingester/ethereum_ingester.py)

tests/integration/test_services_integration.py:83: ImportError
___________ TestIngestionToProcessing.test_agent_message_consumption ___________

self = <test_services_integration.TestIngestionToProcessing object at 0x11d466b50>
sample_blockchain_events = [{'block_number': 18500000, 'event_name': 'TRANSFER', 'from_address': '0xsender12300000000000000000000000000000', 'gas...00001, 'event_name': 'SWAP', 'from_address': '0xmevbot45600000000000000000000000000000', 'gas_price_gwei': 200.0, ...}]

    @pytest.mark.asyncio
    async def test_agent_message_consumption(self, sample_blockchain_events):
        """Test agent consuming and processing messages."""
        processed_events = []
    
        # Mock MEV agent
        from services.mev_agent.mev_agent import MEVWatchAgent
    
        agent = MEVWatchAgent()
    
        # Mock signal publishing
        async def capture_signal(signal):
            processed_events.append({
                'signal_type': signal.signal_type,
                'confidence': signal.confidence_score,
                'addresses': signal.related_addresses
            })
    
        agent._publish_signal = capture_signal
    
        # Process high-gas event (should trigger MEV detection)
        high_gas_event = sample_blockchain_events[1]
        await agent._analyze_transaction(high_gas_event)
    
        # Verify signal was generated
        assert len(processed_events) >= 1
        signal = processed_events[0]
>       assert signal['signal_type'] in ['FRONT_RUNNING', 'HIGH_GAS_ANOMALY']
E       AssertionError: assert 'SANDWICH_ATTACK' in ['FRONT_RUNNING', 'HIGH_GAS_ANOMALY']

tests/integration/test_services_integration.py:123: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    services.mev_agent.mev_agent:mev_agent.py:251 {"service": "mev-watch-agent", "error": "'event_data'", "event": "Error in frontrunning detection", "logger": "services.mev_agent.mev_agent", "level": "error", "timestamp": "2025-07-25T20:44:19.250955Z"}
ERROR    services.mev_agent.mev_agent:mev_agent.py:294 {"service": "mev-watch-agent", "error": "'event_data'", "event": "Error in arbitrage detection", "logger": "services.mev_agent.mev_agent", "level": "error", "timestamp": "2025-07-25T20:44:19.251133Z"}
ERROR    services.mev_agent.mev_agent:mev_agent.py:331 {"service": "mev-watch-agent", "error": "'event_data'", "event": "Error in MEV bot detection", "logger": "services.mev_agent.mev_agent", "level": "error", "timestamp": "2025-07-25T20:44:19.251166Z"}
______________ TestAPIIntegration.test_graphql_api_basic_queries _______________

self = <test_services_integration.TestAPIIntegration object at 0x11d467710>

    def test_graphql_api_basic_queries(self):
        """Test GraphQL API basic functionality."""
        from services.graph_api.graph_api import app
        from fastapi.testclient import TestClient
    
        client = TestClient(app)
    
        # Test schema introspection
        introspection_query = """
        query IntrospectionQuery {
            __schema {
                queryType { name }
                types {
                    name
                    kind
                }
            }
        }
        """
    
        response = client.post("/graphql", json={"query": introspection_query})
        assert response.status_code == 200
    
        data = response.json()
        assert 'data' in data
        assert '__schema' in data['data']
    
        # Verify core types exist
        type_names = [t['name'] for t in data['data']['__schema']['types']]
        expected_types = ['Entity', 'Address', 'Transaction', 'Query']
    
        for expected_type in expected_types:
>           assert expected_type in type_names
E           AssertionError: assert 'Address' in ['Query', 'String', 'Int', 'Mutation', 'Entity', 'Float', ...]

tests/integration/test_services_integration.py:189: AssertionError
______________ TestDatabaseIntegration.test_bigquery_mock_queries ______________

self = <test_services_integration.TestDatabaseIntegration object at 0x11d464790>

    def test_bigquery_mock_queries(self):
        """Test BigQuery query functionality with mocks."""
        with patch('google.cloud.bigquery.Client') as mock_bq:
            mock_client = Mock()
    
            # Mock query result
            mock_result = [
                {'block_number': 18500000, 'tx_count': 150},
                {'block_number': 18500001, 'tx_count': 143},
                {'block_number': 18500002, 'tx_count': 167}
            ]
    
            mock_job = Mock()
            mock_job.result.return_value = [Mock(**row) for row in mock_result]
            mock_client.query.return_value = mock_job
            mock_bq.return_value = mock_client
    
            # Test database helper
>           from services.ingestion.database_helper import BigQueryHelper
E           ModuleNotFoundError: No module named 'services.ingestion.database_helper'

tests/integration/test_services_integration.py:276: ModuleNotFoundError
_______ TestSecurityAndCompliance.test_access_control_policy_enforcement _______

self = <test_services_integration.TestSecurityAndCompliance object at 0x11d465610>

    def test_access_control_policy_enforcement(self):
        """Test access control policy evaluation."""
>       from services.access_control.audit_sink import PolicyEvaluator
E       ImportError: cannot import name 'PolicyEvaluator' from 'services.access_control.audit_sink' (/Users/jadenfix/eth/services/access_control/audit_sink.py)

tests/integration/test_services_integration.py:290: ImportError
__________ TestSecurityAndCompliance.test_audit_logging_functionality __________

self = <test_services_integration.TestSecurityAndCompliance object at 0x11d434950>

    def test_audit_logging_functionality(self):
        """Test comprehensive audit logging."""
        from services.access_control.audit_sink import AuditLogger
    
        logger = AuditLogger()
    
        # Generate test audit entries
        entries = []
    
        # Successful query
        entries.append(logger.log_access(
            user='analyst@company.com',
            resource='transactions_table',
            action='SELECT',
            result='SUCCESS',
            metadata={'rows_returned': 150}
        ))
    
        # Failed access attempt
        entries.append(logger.log_access(
            user='external@badactor.com',
            resource='sensitive_data',
            action='SELECT',
            result='DENIED',
            metadata={'reason': 'unauthorized_user'}
        ))
    
        # Verify audit entries structure
        for entry in entries:
            assert 'user' in entry
            assert 'resource' in entry
            assert 'action' in entry
            assert 'result' in entry
            assert 'timestamp' in entry
>           assert 'session_id' in entry
E           AssertionError: assert 'session_id' in {'action': 'SELECT', 'metadata': {'rows_returned': 150}, 'resource': 'transactions_table', 'result': 'SUCCESS', ...}

tests/integration/test_services_integration.py:351: AssertionError
__________ TestSecurityAndCompliance.test_data_masking_implementation __________

self = <test_services_integration.TestSecurityAndCompliance object at 0x11d434890>

    def test_data_masking_implementation(self):
        """Test data masking for sensitive information."""
        from services.access_control.audit_sink import DataMasker
    
        masker = DataMasker()
    
        # Test data with mixed sensitive and non-sensitive fields
        test_record = {
            'transaction_hash': '0x123abc456def789',
            'from_address': '0xsender123',
            'to_address': '0xrecipient456',
            'user_email': 'user@example.com',
            'phone_number': '+1-555-123-4567',
            'value_usd': 15000.50
        }
    
>       masked_record = masker.mask_sensitive_data(test_record, user_role='analyst')
E       TypeError: DataMasker.mask_sensitive_data() got an unexpected keyword argument 'user_role'

tests/integration/test_services_integration.py:369: TypeError
_________ TestWorkflowIntegration.test_dagster_job_definition_loading __________

self = <test_services_integration.TestWorkflowIntegration object at 0x11d4352d0>

    def test_dagster_job_definition_loading(self):
        """Test loading Dagster job definitions."""
>       from services.workflow_builder.sample_signal import (
            high_value_transfer_monitor,
            suspicious_activity_monitor
        )

tests/integration/test_services_integration.py:386: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
____________ TestWorkflowIntegration.test_custom_workflow_creation _____________

self = <test_services_integration.TestWorkflowIntegration object at 0x11d435390>

    def test_custom_workflow_creation(self):
        """Test dynamic workflow creation."""
>       from services.workflow_builder.sample_signal import build_custom_workflow

tests/integration/test_services_integration.py:401: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
__________ TestWorkflowIntegration.test_workflow_execution_simulation __________

self = <test_services_integration.TestWorkflowIntegration object at 0x11d435810>

    @pytest.mark.asyncio
    async def test_workflow_execution_simulation(self):
        """Test workflow execution with mocked components."""
        import pandas as pd
        from unittest.mock import Mock
>       from services.workflow_builder.sample_signal import (
            fetch_blockchain_data,
            detect_anomalies,
            generate_signal
        )

tests/integration/test_services_integration.py:441: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    Workflow Builder Service - Dagster job definitions for low-code signal building.
    
    Provides visual workflow composition for non-technical users to create
    custom blockchain monitoring and alerting workflows.
    """
    
    import os
    from typing import Dict, List, Any, Optional
    from datetime import datetime, timedelta
    
    from dagster import (
        job, op, Config, In, Out, DynamicOut, DynamicPartitionsDefinition,
        resource, sensor, schedule, asset, AssetMaterialization,
        get_dagster_logger, OpExecutionContext, JobDefinition
    )
>   from dagster_gcp import BigQueryResource, gcp_gcs_resource
E   ImportError: cannot import name 'gcp_gcs_resource' from 'dagster_gcp' (/Users/jadenfix/.pyenv/versions/3.11.3/lib/python3.11/site-packages/dagster_gcp/__init__.py)

services/workflow_builder/sample_signal.py:17: ImportError
________ TestSystemHealthMonitoring.test_health_check_service_discovery ________

self = <test_services_integration.TestSystemHealthMonitoring object at 0x11d436850>

    @pytest.mark.asyncio
    async def test_health_check_service_discovery(self):
        """Test health checking of registered services."""
>       from services.monitoring.health_service import HealthMonitoringService

tests/integration/test_services_integration.py:502: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    System Health and Monitoring Service.
    
    Provides comprehensive health checks, performance metrics, and
    operational insights for the blockchain intelligence platform.
    """
    
    import os
    import asyncio
    import json
    import time
    import logging
    import psutil
    from typing import Dict, List, Optional, Any, Tuple
    from dataclasses import dataclass, asdict
    from datetime import datetime, timedelta
    from enum import Enum
    
    import structlog
    import aiohttp
>   import aioredis
E   ModuleNotFoundError: No module named 'aioredis'

services/monitoring/health_service.py:21: ModuleNotFoundError
______ TestSystemHealthMonitoring.test_metrics_collection_and_aggregation ______

self = <test_services_integration.TestSystemHealthMonitoring object at 0x11d437e50>

    def test_metrics_collection_and_aggregation(self):
        """Test system metrics collection."""
>       from services.monitoring.health_service import MetricsCollector

tests/integration/test_services_integration.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    System Health and Monitoring Service.
    
    Provides comprehensive health checks, performance metrics, and
    operational insights for the blockchain intelligence platform.
    """
    
    import os
    import asyncio
    import json
    import time
    import logging
    import psutil
    from typing import Dict, List, Optional, Any, Tuple
    from dataclasses import dataclass, asdict
    from datetime import datetime, timedelta
    from enum import Enum
    
    import structlog
    import aiohttp
>   import aioredis
E   ModuleNotFoundError: No module named 'aioredis'

services/monitoring/health_service.py:21: ModuleNotFoundError
_________ TestSystemHealthMonitoring.test_alert_generation_and_routing _________

self = <test_services_integration.TestSystemHealthMonitoring object at 0x11d4372d0>

    def test_alert_generation_and_routing(self):
        """Test alert generation from health checks."""
>       from services.monitoring.health_service import (
            AlertManager,
            HealthCheck,
            HealthStatus,
            ServiceType
        )

tests/integration/test_services_integration.py:549: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    """
    System Health and Monitoring Service.
    
    Provides comprehensive health checks, performance metrics, and
    operational insights for the blockchain intelligence platform.
    """
    
    import os
    import asyncio
    import json
    import time
    import logging
    import psutil
    from typing import Dict, List, Optional, Any, Tuple
    from dataclasses import dataclass, asdict
    from datetime import datetime, timedelta
    from enum import Enum
    
    import structlog
    import aiohttp
>   import aioredis
E   ModuleNotFoundError: No module named 'aioredis'

services/monitoring/health_service.py:21: ModuleNotFoundError
________________ TestEthereumIngester.test_event_normalization _________________

self = <test_services.TestEthereumIngester object at 0x11d467f90>

    def test_event_normalization(self):
        """Test transaction event normalization."""
        from services.ethereum_ingester.ethereum_ingester import EventNormalizer
    
        normalizer = EventNormalizer()
    
        raw_transaction = {
            'hash': '0x123abc',
            'from': '0xsender',
            'to': '0xrecipient',
            'value': 1000000000000000000,  # 1 ETH in wei
            'gasPrice': 20000000000,  # 20 gwei
            'gas': 21000,
            'gasUsed': 21000
        }
    
        normalized = normalizer.normalize_transaction(raw_transaction, 18500000)
    
        assert normalized['event_name'] == 'TRANSACTION'
        assert normalized['chain_id'] == 1
        assert normalized['block_number'] == 18500000
        assert normalized['from_address'] == '0xsender'
        assert normalized['to_address'] == '0xrecipient'
        assert normalized['value_eth'] == 1.0
>       assert normalized['gas_price_gwei'] == 20.0
E       KeyError: 'gas_price_gwei'

tests/unit/test_services.py:39: KeyError
_______________ TestEthereumIngester.test_value_usd_calculation ________________

self = <test_services.TestEthereumIngester object at 0x11d4644d0>

    def test_value_usd_calculation(self):
        """Test USD value calculation."""
>       from services.ethereum_ingester.ethereum_ingester import ValueCalculator
E       ImportError: cannot import name 'ValueCalculator' from 'services.ethereum_ingester.ethereum_ingester' (/Users/jadenfix/eth/services/ethereum_ingester/ethereum_ingester.py)

tests/unit/test_services.py:43: ImportError
__________________ TestEthereumIngester.test_block_processing __________________

self = <test_services.TestEthereumIngester object at 0x11d467890>

    @pytest.mark.asyncio
    async def test_block_processing(self):
        """Test block processing logic."""
        with patch('services.ethereum_ingester.ethereum_ingester.Web3') as mock_web3:
            from services.ethereum_ingester.ethereum_ingester import EthereumIngester
    
            # Mock Web3 instance
            mock_instance = Mock()
            mock_instance.eth.get_block.return_value = Mock(
                number=18500000,
                timestamp=1640995200,
                transactions=[Mock(hash='0x123', **{'from': '0xabc'})]
            )
            mock_web3.return_value = mock_instance
    
            ingester = EthereumIngester()
    
            # Mock the publishing method
            published_events = []
            async def mock_publish(event):
                published_events.append(event)
            ingester._publish_event = mock_publish
    
            await ingester._process_block(18500000)
    
            assert len(published_events) > 0
>           assert published_events[0]['block_number'] == 18500000
E           TypeError: 'ChainEvent' object is not subscriptable

tests/unit/test_services.py:78: TypeError
------------------------------ Captured log call -------------------------------
ERROR    services.ethereum_ingester.ethereum_ingester:ethereum_ingester.py:149 {"service": "ethereum-ingester", "tx_hash": "0x123", "error": "'Mock' object is not iterable", "event": "Error processing transaction", "logger": "services.ethereum_ingester.ethereum_ingester", "level": "error", "timestamp": "2025-07-25T20:44:19.696625Z"}
=========================== short test summary info ============================
SKIPPED [1] tests/e2e/tier2/test_t2_a_real_service_integration.py:393: GraphQL endpoint returned 404
FAILED tests/e2e/test_comprehensive.py::TestLayer1Ingestion::test_ethereum_ingestion_pipeline
FAILED tests/e2e/test_comprehensive.py::TestLayer2SemanticFusion::test_ontology_graphql_api
FAILED tests/e2e/test_comprehensive.py::TestLayer3IntelligenceAgentMesh::test_vertex_ai_pipeline_mock
FAILED tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_graphql_api_endpoints
FAILED tests/e2e/test_comprehensive.py::TestLayer4APIVoiceOps::test_voice_ops_integration
FAILED tests/e2e/test_comprehensive.py::TestLayer5UXWorkflowBuilder::test_dagster_workflow_execution
FAILED tests/e2e/test_comprehensive.py::TestLayer5UXWorkflowBuilder::test_custom_workflow_builder
FAILED tests/e2e/test_comprehensive.py::TestLayer6SystemIntegration::test_full_pipeline_integration
FAILED tests/e2e/test_comprehensive.py::TestLayer6SystemIntegration::test_health_monitoring_integration
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_encryption_at_rest
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_gdpr_compliance
FAILED tests/e2e/test_comprehensive.py::TestSecurityCompliance::test_soc2_audit_trail
FAILED tests/e2e/tier0/test_t0_a_basic_ingestion.py::TestIngestToBigQuery::test_ingest_with_pubsub_simulation
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_simple_graph_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_path_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_aggregation_query
FAILED tests/e2e/tier0/test_t0_c_graph_queries.py::TestNeo4jGraphQueries::test_graph_export_format
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_dashboard_loads_without_crash
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_graph_visualization_endpoint
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_health_check_endpoint
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_api_error_handling
FAILED tests/e2e/tier0/test_t0_d_ui_rendering.py::TestUIRendering::test_static_assets_loading
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_pubsub_to_bigquery_pipeline
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_high_volume_ingestion
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_data_validation_and_filtering
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_streaming_ingestion_latency
FAILED tests/e2e/tier1/test_t1_a_realtime_ingestion.py::TestRealTimeIngestion::test_duplicate_detection
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_bigquery_to_neo4j_sync
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_neo4j_to_bigquery_sync
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_bidirectional_consistency_check
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_real_time_sync_latency
FAILED tests/e2e/tier1/test_t1_b_bidirectional_sync.py::TestBidirectionalSync::test_sync_conflict_resolution
FAILED tests/e2e/tier2/test_t2_a_real_service_integration.py::TestRealServiceIntegration::test_websocket_endpoints
FAILED tests/e2e/tier2/test_t2_b_v3_patches_integration.py::TestV3PatchesIntegration::test_patch_4_autonomous_action_executor
FAILED tests/integration/test_services_integration.py::TestIngestionToProcessing::test_pubsub_message_flow
FAILED tests/integration/test_services_integration.py::TestIngestionToProcessing::test_agent_message_consumption
FAILED tests/integration/test_services_integration.py::TestAPIIntegration::test_graphql_api_basic_queries
FAILED tests/integration/test_services_integration.py::TestDatabaseIntegration::test_bigquery_mock_queries
FAILED tests/integration/test_services_integration.py::TestSecurityAndCompliance::test_access_control_policy_enforcement
FAILED tests/integration/test_services_integration.py::TestSecurityAndCompliance::test_audit_logging_functionality
FAILED tests/integration/test_services_integration.py::TestSecurityAndCompliance::test_data_masking_implementation
FAILED tests/integration/test_services_integration.py::TestWorkflowIntegration::test_dagster_job_definition_loading
FAILED tests/integration/test_services_integration.py::TestWorkflowIntegration::test_custom_workflow_creation
FAILED tests/integration/test_services_integration.py::TestWorkflowIntegration::test_workflow_execution_simulation
FAILED tests/integration/test_services_integration.py::TestSystemHealthMonitoring::test_health_check_service_discovery
FAILED tests/integration/test_services_integration.py::TestSystemHealthMonitoring::test_metrics_collection_and_aggregation
FAILED tests/integration/test_services_integration.py::TestSystemHealthMonitoring::test_alert_generation_and_routing
FAILED tests/unit/test_services.py::TestEthereumIngester::test_event_normalization
FAILED tests/unit/test_services.py::TestEthereumIngester::test_value_usd_calculation
FAILED tests/unit/test_services.py::TestEthereumIngester::test_block_processing
!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 50 failures !!!!!!!!!!!!!!!!!!!!!!!!!!
50 failed, 60 passed, 1 skipped, 42 warnings in 65.35s (0:01:05)
